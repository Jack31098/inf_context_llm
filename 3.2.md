### 3.2 Token-Level Memory Management: Summary Trigger & Trimmer

At the **block level**, the system must decide:

1. **When** to cut off the current stream and archive a block (summary trigger).  
2. **Within that block**, **which tokens’ KV** remain on GPU and which can be safely evicted to host (Trimmer).

This section defines the **token-level mechanisms** that sit *inside* each block:

- A **learned summary trigger** (`<SUMMARIZE>`) that decides when a block should be closed and archived.  
- A **Trimmer module** that learns, under a fixed KV budget, **which tokens are worth keeping** for future computation.

Q-Former, block-level summaries, and block handlers are covered later in **Section 3.3**.

---

#### 3.2.1 Summary Trigger & Block Segmentation

The **summary trigger** is the joint between the **continuous token stream** and the **block-based memory system**.

We introduce a special control token:

- `<SUMMARIZE>` – a learned “segment boundary” marker.

When `<SUMMARIZE>` fires at time step `t`:

1. The **current block** `[t_block_start, ..., t]` is finalized.  
2. The block’s hidden states and KV become eligible for:
   - Trimming (token-level KV selection).  
   - Archival (block-level semantic compression, see Section 3.3).  
3. The Memory OS:
   - Updates anchors (Global / Local).  
   - Starts a **new block** from `t+1`.

The summary trigger is learned but backed by simple heuristics to guarantee robustness.

---

##### 3.2.1.1 Supervised Summary Trigger

We provide **explicit supervision** for where `<SUMMARIZE>` *should* appear.

**Data construction:**

For each long sequence (dialogue, document, code), we generate **segment boundaries** by:

- Human / heuristic / LLM-based topic segmentation:
  - Discourse boundaries (end of an answer, end of a paragraph).  
  - Topic shifts (“now let’s talk about X”).  
  - Completion of a logical unit (e.g., a function definition, a resolved QA pair).

We then insert synthetic labels:

- At each boundary position `t`, the target next token is `<SUMMARIZE>`.  
- At non-boundary positions, `<SUMMARIZE>` is **not** in the target set.

**Training:**

- The base LLM (optionally with a small LoRA head) is trained with **token-level CE** to predict `<SUMMARIZE>` at the correct boundaries.  
- To avoid over-firing:
  - We can down-weight positive examples, or  
  - Add a **penalty** on excessive `<SUMMARIZE>` frequency.

At inference time, we do **not** force `<SUMMARIZE>` to appear at exactly those positions; instead:

- The model learns a **distribution over “segmentation desirability”**,  
- The Memory OS uses this plus simple heuristics (see below) to decide whether to actually cut.

---

##### 3.2.1.2 Heuristic Fallbacks & Safety Rails

To avoid pathological behaviors (never summarizing / summarizing every few tokens), the Memory OS adds simple, **non-learned fallback rules**:

- **Max block length:**  
  - If the current block exceeds `L_max` tokens and no `<SUMMARIZE>` was triggered,  
    - force a segment cut.  
- **Min block length:**  
  - If a recent `<SUMMARIZE>` was fired and the block is shorter than `L_min`,  
    - ignore new `<SUMMARIZE>` triggers to avoid excessively fragmented blocks.
- **Time-based triggers:**  
  - In interactive dialogue, if a **turn** ends and the block is reasonably long,  
    - we can optionally treat it as a soft segmentation signal.

These rules make sure the system **always** produces reasonably sized blocks, even if the learned trigger is imperfect.

---

#### 3.2.2 Trimmer: Learning Which Tokens’ KV to Keep

Once a block is cut, we must decide **which tokens’ KV states remain on GPU** and which can be safely evicted to host.

Instead of:

- Keeping a naive sliding window, or  
- Only preserving the first 4 tokens as anchors,

we train a **Trimmer module**:

> A **soft Top-K KV selector** that, given the block’s hidden states, learns which tokens are truly important under a fixed KV budget.

**Inputs:**

- Block hidden states:  
  \[
  H = [h_1, h_2, \dots, h_T], \quad h_t \in \mathbb{R}^{d_\text{model}}
  \]
- An optional **summary condition**:
  - Either the hidden state at `<SUMMARIZE>` position,  
  - Or a pooled representation of the block:
    \[
    h_\text{sum} = \text{Pool}(H)
    \]

**Output:**

- A scalar **importance logit** `l_t` for each token.  
- During training, used to generate **soft masks**;  
- During inference, converted into **hard Top-K selections**.

---

##### 3.2.2.1 Scoring Network

The Trimmer is implemented as a lightweight scorer network:

\[
\tilde{h}_t = \text{LayerNorm}(h_t \oplus h_\text{sum}) \\
l_t = w^\top \sigma(W \tilde{h}_t + b) + c
\]

where:

- `⊕` is concatenation or a simple fusion (e.g., gated addition).  
- `σ` is a non-linear activation (e.g., GELU or SiLU).  
- `l_t` is a scalar logit per token.

In matrix form:

- For block length `T`:
  - `L_trim = [l_1, ..., l_T]` is a `T`-dim vector.

---

##### 3.2.2.2 Soft-to-Hard Top-K via Gumbel-Softmax

We use **Gumbel-Softmax + Straight-Through** to allow exploration of different retention patterns while still backpropagating gradients.

**Training-time sampling:**

1. For each token `t`, sample Gumbel noise `g_t`.  
2. Compute noisy scores:
   \[
   \hat{l}_t = l_t + g_t
   \]
3. For each sample `n` (we can take `N` different masks per step):
   - Select Top-K tokens according to `\hat{l}_t` to form a **hard mask**:
     \[
     M^\text{hard}_n \in \{0, 1\}^T, \quad \sum_t M^\text{hard}_{n, t} = K
     \]
4. Build **soft masks** with Straight-Through trick:
   \[
   M^\text{soft}_n = \text{stop\_grad}(M^\text{hard}_n - \text{softmax}(L_\text{trim})) + \text{softmax}(L_\text{trim})
   \]

**Interpretation:**

- Forward pass:
  - Uses **hard Top-K** selection → exactly `K` tokens’ KV survive.  
- Backward pass:
  - Gradients flow through the **softmax** part, giving dense learning signals.

---

#### 3.2.3 Training the Trimmer: Teacher–Student Distillation

We treat the **full-KV model** as a **Teacher**, and the **trimmed-KV model** (with Trimmer masks) as a **Student**.

The basic setup:

- **Teacher:**  
  - Runs on the full block with **no trimming**,  
  - Produces logits `Y_teacher` and attention patterns `A_teacher`.

- **Student:**  
  - Applies mask `M_n` to KV inside the block:
    - Masked positions’ KV are zeroed or dropped.  
  - Continues generation from the same point, producing:
    - logits `Y_student`  
    - attention `A_student` (optional)

We define the loss per mask `n` as:

\[
\mathcal{L}_\text{trim}^{(n)} =
\alpha \cdot \mathcal{L}_\text{CE}(Y_{student}, Y_{target})
+ \beta \cdot \mathcal{L}_\text{KL}(Y_{student} \parallel Y_{teacher})
+ \gamma \cdot \mathcal{L}_\text{Attn-KL}(A_{student} \parallel A_{teacher})
\]

where:

- `CE` is standard next-token prediction loss.  
- `KL` between student and teacher logits encourages the student to **match the full-context distribution**, not just hit the correct argmax.  
- `Attn-KL` (optional) encourages the model to preserve important attention structures.

The total Trimmer loss:

\[
\mathcal{L}_\text{trim} = \frac{1}{N} \sum_{n=1}^N \mathcal{L}_\text{trim}^{(n)}
\]

This enforces:

- Under a fixed KV budget `K`, masks that drop truly important tokens will suffer higher CE/KL loss.  
- The Trimmer will learn to **assign higher scores** to tokens whose KV actually matters.

---

##### 3.2.3.1 Physical vs Logical Trimming

- **During training:**
  - Trimming is **logical**:
    - We do not physically move KV to host RAM,  
    - We simply zero out / mask attention to dropped positions within GPU memory.  
  - This keeps training simple and differentiable.

- **During inference:**
  - Trimming becomes **physical**:
    - Tokens selected by the Trimmer remain in GPU KV.  
    - Evicted tokens’ KV are:
      - Either discarded, or  
      - Moved to host / archived, potentially leaving behind **tombstones**.

This decoupling allows the Trimmer to be trained purely as a **mask predictor**, without entangling low-level PCIe or KV-layout issues.

---

#### 3.2.4 Joint Adaptation with LoRA (Optional)

Empirically, large LLMs can be **brittle** under aggressive KV trimming:

- Some layers are highly sensitive to losing certain tokens.  
- Naively trimming can cause quality collapse in rare but severe cases.

To mitigate this, we optionally:

- Attach a small **LoRA** to the base LLM.  
- Train **Trimmer and LoRA jointly**:

  - Trimmer learns to propose **good masks** under the KV budget.  
  - LoRA lets the LLM **adapt to the new operating regime**:
    - “KV is no longer dense; it has been filtered.”  
    - “Prefix KV (from future blocks / summaries) may be injected.”

Training schedule:

1. **Stage 1 – Frozen LLM, train only Trimmer:**
   - On top of a simple baseline (e.g., first-4 anchor + rolling window).  
   - Get a stable Trimmer that works reasonably well.

2. **Stage 2 – Unfreeze LoRA, joint training:**
   - LoRA helps recover quality lost due to trimming,  
   - Especially for long contexts and edge cases.

---

#### 3.2.5 Inference-Time Behavior (Within a Block)

Putting it all together, for each block:

1. **Online generation:**
   - LLM generates tokens with the Memory OS watching for `<SUMMARIZE>` and anchor rules.  
   - KV grows until:
     - `<SUMMARIZE>` fires, or  
     - Block length reaches `L_max`.

2. **Block finalization:**
   - Once a block is closed:
     - The Trimmer scores all tokens in `[t_block_start, ..., t_block_end]`.  
     - Top-K tokens (anchors + high-score tokens) are marked “to keep”.  
     - Others:
       - Are evicted to host,  
       - Or dropped, leaving behind **tombstone tokens** pointing to archived content.

3. **Hand-off to block-level memory:**
   - The block is passed to the **Q-Former and block-level semantic indexing** (Section 3.3).  
   - At this point, the Trimmer’s work is done; the block enters the **long-term memory pipeline**.

In summary:

- **3.2 (this section)** defines **how we segment and thin out token-level KV** before turning a block into a memory unit.  
- **3.3** will define how that memory unit is compressed, tagged, and indexed for later retrieval.
