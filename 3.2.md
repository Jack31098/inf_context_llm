### 3.2 Token-Level Memory Management: Summary Trigger & Trimmer

At the **block level**, the system must decide:

1. **When** to cut off the current stream and archive a block (summary trigger).  
2. **Within that block**, **which tokens’ KV** remain on GPU and which can be safely evicted to host (Trimmer).

This section defines the **token-level mechanisms** that sit *inside* each block:

- A **learned summary trigger** (`<SUMMARIZE>`) that decides when a block should be closed and archived.  
- A **Trimmer module** that learns, under a fixed KV budget, **which tokens are worth keeping** for future computation.

> Q-Former, block-level summaries, and block handlers are covered later in **Section 3.3**.

---

#### 3.2.1 Summary Trigger & Block Segmentation

The **summary trigger** is the junction between the **continuous token stream** and the **block-based memory system**.

We introduce a special control token:

- `<SUMMARIZE>` – a learned “segment boundary” marker.

When `<SUMMARIZE>` fires at time step `t`:

1. The **current block** `[t_block_start, ..., t]` is finalized.  
2. The block’s hidden states and KV become eligible for:
   - Trimming (token-level KV selection).  
   - Archival (block-level semantic compression, see Section 3.3).  
3. The Memory OS:
   - Updates anchors (Global / Local).  
   - Starts a **new block** from `t+1`.

The summary trigger is learned but backed by simple heuristics to guarantee robustness.

---

##### 3.2.1.1 Supervised Summary Trigger

We provide **explicit supervision** for where `<SUMMARIZE>` *should* appear.

**Data construction**

For each long sequence (dialogue, document, code), we generate **segment boundaries** by:

- Human / heuristic / LLM-based topic segmentation:
  - Discourse boundaries (end of an answer, end of a paragraph).  
  - Topic shifts (“now let’s talk about X”).  
  - Completion of a logical unit (e.g., a function definition or a resolved QA pair).

We then insert synthetic labels:

- At each boundary position `t`, the target next token is `<SUMMARIZE>`.  
- At non-boundary positions, `<SUMMARIZE>` is **not** in the target set.

**Training**

- The base LLM (optionally with a small LoRA head) is trained with **token-level CE** to predict `<SUMMARIZE>` at the correct boundaries.  
- To avoid over-firing:
  - We down-weight positive `<SUMMARIZE>` examples, or  
  - Add a simple **frequency penalty** in the loss.

At inference time, the model learns a **probability field over “should I summarize here?”**; the Memory OS then applies additional constraints.

---

##### 3.2.1.2 Heuristic Fallbacks & Safety Rails

To avoid pathological behaviors (never summarizing / summarizing every few tokens), the Memory OS adds simple, **non-learned fallback rules**:

- **Max block length**  
  - If the current block exceeds `L_max` tokens and no `<SUMMARIZE>` fired,  
    - force a segment cut.

- **Min block length**  
  - If `<SUMMARIZE>` fired too soon and the block is shorter than `L_min`,  
    - ignore new `<SUMMARIZE>` triggers for a short cooldown window.

- **Turn-based triggers (dialogue)**  
  - In conversational settings, when a **user turn** ends and the block is reasonably long,  
    - we can treat it as a soft segmentation hint.

These rules guarantee that the system **always** produces reasonably sized blocks, even if the learned trigger is imperfect.

---

#### 3.2.2 Trimmer: Learning Which Tokens’ KV to Keep

Once a block is cut, we must decide **which tokens’ KV states remain on GPU** and which can be safely evicted to host.

Instead of:

- Keeping a naive sliding window, or  
- Only preserving the first 4 tokens as anchors,

we train a **Trimmer module**:

> A **soft Top-K KV selector** that, given the block’s hidden states, learns which tokens are truly important under a fixed KV budget.

**Inputs**

- Block hidden states:  
  \[
  H = [h_1, h_2, \dots, h_T], \quad h_t \in \mathbb{R}^{d_\text{model}}
  \]
- An optional **summary condition**:
  - Either the hidden state at `<SUMMARIZE>` position,  
  - Or a pooled representation of the block:
    \[
    h_\text{sum} = \text{Pool}(H)
    \]

**Output**

- A scalar **importance logit** `l_t` for each token.  
- During training, these logits generate **soft masks**.  
- During inference, they are converted into **hard Top-K selections**.

---

##### 3.2.2.1 Scoring Network

The Trimmer is implemented as a lightweight scorer network:

\[
\tilde{h}_t = \text{LayerNorm}(h_t \oplus h_\text{sum}) \\
l_t = w^\top \sigma(W \tilde{h}_t + b) + c
\]

where:

- `⊕` is concatenation or a simple fusion (e.g., gated addition).  
- `σ` is a non-linear activation (e.g., GELU / SiLU).  
- `l_t` is a scalar logit per token.

In matrix form, for block length `T`:

- `L_trim = [l_1, ..., l_T] ∈ ℝ^T`.

---

##### 3.2.2.2 Hard Gumbel Top-K (Fixed Budget)

In the **first version**, the KV budget is **fixed by heuristic**, not learned:

- We set a rule-based `K = K(T)`:
  - e.g., `K = K0 + α · log(T)` or a simple piecewise constant schedule.  
  - Typical choice:
    - Always keep:
      - global anchors  
      - local topic anchors
    - plus a small quota of **learned important tokens** from the Trimmer.

The selection mechanism:

- **Training time:**
  - Add Gumbel noise to `L_trim`:
    \[
    \hat{l}_t = l_t + g_t
    \]
  - Take **hard Top-K** according to `\hat{l}_t`:
    \[
    M^\text{hard}_t = 
    \begin{cases}
      1, & \text{if } t \text{ in Top-K}(\hat{l}) \\
      0, & \text{otherwise}
    \end{cases}
    \]
  - Use Straight-Through to define a soft version `M^soft` for backprop.

- **Inference time:**
  - No randomness:
    - Directly pick Top-K tokens from `L_trim`.  
  - Only those K tokens’ KV remain on GPU; others are candidates for eviction / tombstones.

> Later work can relax this to **learning K** (via a sparsity penalty / budget regularizer).  
> Phase 1 deliberately uses a **simple, rule-based K** to keep the system stable.

---

#### 3.2.3 Teacher–Student Training: Adversarial Compression Under a Fixed Budget

The Trimmer is trained in a **teacher–student distillation** setup with a built-in **adversarial tension**:

- The **Teacher (SFT LLM)** wants to use **as much context as possible** to reduce loss.  
- The **Trimmer** is forced to operate under a **small K** (aggressive compression),  
  and must learn to pick **only the most necessary tokens**.

We do not explicitly optimize a game between two agents, but the training regime is **implicitly adversarial**:

- Data is constructed to **punish lazy trimming** (dropping important tokens).  
- The KV budget `K` is set small enough that the Trimmer *must* learn non-trivial selection.

---

##### 3.2.3.1 Training Setup

For each training example:

- There is a block `B` with tokens `[1..T]`.  
- We choose a generation position inside `B` or shortly after it.  
- We run:

1. **Teacher run (full KV)**
   - LLM runs on full context (no trimming).  
   - Produces:
     - logits `Y_teacher` for next token(s)  
     - optional attention maps `A_teacher`.

2. **Student run (trimmed KV)**
   - Apply Trimmer’s hard mask `M^hard` (Top-K) to KV inside block `B`:
     - `M_t = 0` → KV for token `t` is masked / dropped.  
     - `M_t = 1` → KV retained.  
   - LLM continues from same position with trimmed KV.  
   - Produces:
     - logits `Y_student`  
     - attention `A_student` (optional)

**Loss per mask:**

\[
\mathcal{L}_\text{trim} =
\alpha \cdot \mathcal{L}_\text{CE}(Y_{student}, Y_{target})
+ \beta \cdot \mathcal{L}_\text{KL}(Y_{student} \parallel Y_{teacher})
+ \gamma \cdot \mathcal{L}_\text{Attn-KL}(A_{student} \parallel A_{teacher})
\]

- `CE` ensures the trimmed model still hits the correct tokens.  
- `KL` ensures its **logit distribution** stays close to the full-context teacher.  
- `Attn-KL` (optional) propagates a soft prior from teacher attention.

Because `K` is small, any mask that discards essential tokens will incur **high CE/KL** → gradients force the Trimmer to raise scores on such tokens.

---

##### 3.2.3.2 Sample Construction: Trimming Scenarios (Especially Topic Switches)

To make the Trimmer actually learn **semantically meaningful selection**, we need **stress-test scenarios** in training, especially around **topic changes**.

We deliberately construct at least three families of trimming scenarios:

1. **Intra-topic redundancy trimming (easy mode)**  
   - Long stretches of filler text (e.g., verbose explanations, small talk),  
   - Only a few tokens truly affect the next prediction (definitions, entities, key steps).  
   - Goal:
     - Trimmer learns to **drop redundant filler** while keeping structural anchors.

2. **Cross-topic boundaries (topic switch) — “forget the right past”**  
   - Training sequences are composed of:
     - **Older topic A**
     - then a clear switch to **topic B**, with its own internal definitions.
   - The target tokens belong purely to topic B.
   - Under full KV:
     - Teacher may still attend into topic A occasionally (long-range attention tails).  
   - Under trimming:
     - Trimmer is encouraged to:
       - Keep B’s **local anchors** and recent relevant tokens.  
       - Aggressively drop most of A, except:
         - global anchors  
         - any **bridging tokens** reused in B (e.g., shared entity, reused variable).

   - Data hint:
     - We can tag timestamps / topic IDs in metadata, and construct situations where:
       - A naive “keep all history” policy is wasteful,  
       - The optimal strategy is to **mostly forget previous topics**, but **not entirely**.

3. **Adversarial long-range dependence (hard mode)**  
   - Sequences with multiple “chapters”:
     - A1, A2, B1, B2, ...,  
     - where the final question depends on **A1 + B2**, not the middle noise.  
   - Construct answers / next tokens that:
     - Are impossible without reading specific tokens in **older segments**.  
   - Here:
     - Teacher uses full KV and gets low loss.  
     - Any Trimmer that drops those few key tokens gets **punished**.

In all these scenarios:

- The **Teacher** is free to use all tokens.  
- The **Trimmer** is “attacked” by the data:  
  - sequences are constructed so that **only very specific tokens** matter,  
  - but the KV budget `K` is kept small,  
  - so the Trimmer must learn to identify **semantic / structural pivot tokens** rather than just “recent ones”.

This realizes the “adversarial” intuition:

> - Trimmer is forced to **keep as few tokens as possible** (small K, hard Top-K).  
> - The SFT / teacher setup **tries to expose as many useful dependencies as possible**, so dropping the wrong tokens is heavily penalized.

---

##### 3.2.3.3 Logical vs Physical Trimming

- **During training:**  
  - Trimming is **logical**:
    - We mask / zero out certain KV entries inside GPU memory.  
    - We do not move KV across PCIe.  
  - This keeps the training loop simple and differentiable.

- **During inference:**  
  - Trimming becomes **physical**:
    - Tokens selected by the Trimmer remain as live KV on GPU.  
    - Remaining tokens:
      - Either have their KV evicted to host,  
      - Or are discarded, leaving behind **tombstones** for future block-level retrieval.

This separation allows us to optimize purely for **information selection**, independently of systems details.

---

#### 3.2.4 Joint Adaptation with LoRA (Optional)

Empirically, large LLMs can be **brittle** under aggressive KV trimming:

- Some layers are highly sensitive to losing certain tokens.  
- Naive trimming can cause quality collapse in long-tail cases.

To mitigate this, we optionally:

- Attach a small **LoRA** to the base LLM.  
- Train **Trimmer and LoRA jointly**:

  - Trimmer learns to propose good masks under the KV budget.  
  - LoRA lets the LLM adapt to the new operating regime:
    - “KV is no longer dense; it has been filtered.”  
    - “Prefix KV may be injected from summaries / retrieved blocks.”

**Training schedule**

1. **Stage 1 – Frozen LLM, train only Trimmer**
   - On top of a simple baseline (first-4 anchors + rolling window).  
   - Get a stable Trimmer that works under a rule-based `K`.

2. **Stage 2 – Unfreeze LoRA, joint training**
   - LoRA helps recover performance lost due to trimming,  
   - Especially on:
     - very long contexts  
     - topic-switch cases  
     - adversarial long-range dependencies.

---

#### 3.2.5 Inference-Time Behavior (Within a Block)

For each block, the token-level pipeline is:

1. **Online generation**
   - LLM generates tokens while the Memory OS monitors:
     - `<SUMMARIZE>` predictions  
     - anchor policies  
   - KV accumulates until:
     - `<SUMMARIZE>` fires, or  
     - block length reaches `L_max`.

2. **Block finalization**
   - When the block is closed:
     - The Trimmer scores `H = [h_1, ..., h_T]`.  
     - Hard Top-K (rule-based `K`) selects:
       - global anchors  
       - local topic anchors  
       - the highest-scoring tokens per block.  
     - Other tokens:
       - have their KV evicted or dropped,  
       - optionally leaving **tombstones** that mark where information was archived.

3. **Hand-off to block-level memory (Section 3.3)**
   - The finalized block (with its selected KV + tombstones) is handed off to the Q-Former and block-level indexing pipeline.  
   - 3.2 ends here: it only governs **when to cut** and **what KV to keep** at the token level.

In summary:

- **3.2** defines a **write-side token-level game**:
  - A learned summary trigger decides **where to cut**.  
  - A Trimmer, under a fixed `K`, learns **which tokens deserve to survive**.  
- Block-level compression and retrieval, built on top of these trimmed blocks, are the topic of **Section 3.3**.
