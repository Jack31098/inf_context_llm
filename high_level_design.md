# High-Performance Infinite-Context Inference System

**Design Doc: Host-Resident Memory & Sparse Semantic Retrieval**

---

## 1. Executive Summary

This project aims to build an **‚Äúinfinite-context‚Äù inference system** that supports **long-term conversational memory of 1M+ tokens** on a **single 4√ó MI100 node + large host RAM (~1024GB)**.

The core innovation is to decouple **Compute** and **Memory**:

- **Storage layer:** Use host RAM to store massive KV caches, removing the GPU HBM/VRAM bottleneck.  
- **Indexing layer:** Introduce a Q-Former for semantic compression and vector indexing.  
- **Compute layer:** Use RoPE dynamic re-phasing to enable seamless retrieve-and-inject context stitching.

The end goal is a **memory OS** for mid-sized LLMs (e.g., Qwen3-8B) that can:

- Decide **what to remember / what to forget** at the **token level**  
- Organize history into **semantic memory blocks**  
- Retrieve relevant blocks later via **differentiable neural retrieval**, not brute-force scanning

---

## 2. System Architecture

The system adopts a layered architecture with **separation of storage and compute** (read/write separation).

### 2.1 Memory OS Controller (Control Layer)

This is the **‚Äúbrain‚Äù** of the system, residing inside the LLM inference loop. It intercepts special tokens and orchestrates lower-level resources.

**Responsibilities**

- **State machine management**  
  Switch between:
  - `Generating` ‚Äì normal token generation  
  - `Archiving` ‚Äì memory write / compaction  
  - `Retrieving` ‚Äì memory recall and context augmentation  

- **Signal interception**  
  Listen for special control tokens:
  - `<SUMMARIZE>` ‚Äì triggers archiving of recent context into memory blocks  
  - `<RETRIEVE>` ‚Äì triggers block-level retrieval  

- **VRAM/HBM management**  
  Handle KV cache:
  - **Split** ‚Äì partition KV into manageable blocks  
  - **Evict** ‚Äì migrate old KV to host or archive  
  - **Concat** ‚Äì stitch KV segments (including recalled blocks) into the current context

#### Anchor-Aware Eviction Policy

To address the semantic loss caused by simple sliding windows in StreamingLLM, we introduce a **three-tier GPU-residency strategy**:

1. **Global Sinks (physical anchors)**  
   - **Keep:** Positions `0‚Äì4` (System prompt + BOS)  
   - These tokens serve as the **absolute reference frame** for the model.

2. **Local Anchors (semantic anchors)**  
   - **Keep:** When the Segmentation Policy detects a **new topic**, lock the first `K` tokens of that topic.  
   - These act as **semantic ‚Äúchapter headers‚Äù**.

3. **Rolling Buffer (working memory)**  
   - **Keep:** The most recently generated `M` tokens.  
   - Provides short-term working context for local reasoning.

4. **Tombstones (memory tombstones)**  
   - **Keep:** For each evicted token span, retain a **single residual token** generated by the Q-Former.  
   - This acts as a placeholder for ‚Äúforgotten information‚Äù and helps guide **retrieval decisions** (e.g., via attention to tombstones).

---

### 2.2 Semantic Indexing Engine (Semantic Layer)

Responsible for transforming unstructured KV blocks into **searchable vectors**, and mapping **query intent** into the same vector space.

- **Write-side (Compressor):**
  - Based on a **Q-Former**.
  - Compress large KV blocks (e.g., 4096 tokens) into a compact set of **summary / Detail embeddings** (e.g., 32 vectors).

- **Read-side (Projector):**
  - Based on a **Gated MLP**.
  - Map LLM hidden states (e.g., at `<RETRIEVE>` positions) into the **summary embedding space**.

This provides a **bridge** between the LLM‚Äôs internal representation and the external memory index.

---

### 2.3 Hierarchical Storage (Storage Layer)

The system uses a **multi-tier storage hierarchy** to balance speed, capacity, and semantic richness across **4√ó MI100 HBM** and **~1TB host RAM**.

We explicitly distinguish between:

- **What we retrieve by default** (semantic, compact, interpretable)  
- **What we keep as an optional archive** (full raw KV, lossless but heavy)

#### L1: Hot Index (GPU HBM / Host RAM)

- **Stores:**
  - **Index embeddings** `H` (from the Index Distiller in Phase 2)  
  - **Centroids / routing structures** (for DNII / HNSW)

- **Role:**
  - Supports **high-frequency vector retrieval** given a query embedding `Q`.  
  - Lives partially on GPU, partially in pinned host memory for fast access.

#### L2: Semantic Payload (Host RAM ‚Äî Default Recall Path)

- **Stores:**
  - **Detail Tokens** `D` produced by the frozen Q-Former (Phase 1) for each block.

- **Role:**
  - This is the **primary payload** for memory recall.  
  - At retrieval time, the system **loads `D` and injects them as a soft prompt** into the LLM.  
  - `D` are:
    - Compact (e.g., 32 tokens per 4K block)  
    - **Reasoning-aware** (trained via Explanation SFT + Reconstruction)  
    - Suitable for **semantic, interpretable recall**.

#### L3: Raw KV Archive (Host RAM / SSD ‚Äî Optional Fallback)

- **Stores (optional):**
  - **De-rotated (unphased) raw KV cache** in FP8 format for each archived block.

- **Role:**
  - Acts as an **optional, lossless backup** for cases where:
    - Verbatim quoting is required (‚Äúshow me the exact original text‚Äù)  
    - Extremely fine-grained reconstruction is needed beyond what `D` can capture.
  - Loaded via PCIe **only on demand**, and spliced back into GPU KV via RoPE re-phasing.

- **Default behavior:**
  - **Not used in the normal retrieval path.**  
  - The default infinite-context workflow uses **L2 Detail Tokens** only, to stay consistent with the **interpretation-driven architecture**.

---

### 2.4 Cognitive Memory Graph (Topology Layer)

**Structure:** A dynamically maintained graph that acts as a **higher-level structure** over L1/L2.

- **Nodes:**
  - **Real Nodes:** Summaries of concrete memory blocks (e.g., block-level Detail Tokens or their centroids).  
  - **Virtual Nodes:** Dynamically created **cluster centers** (topic centroids, timelines, entities).

- **Edges:**
  - **Temporal:** `Next / Prev` relations along time.  
  - **Hierarchical:** Belonging relations (`Real Node ‚Üí Virtual Node`).  

In later phases, this graph becomes the substrate for **macro-level navigation** (RL-based ‚ÄúWalker‚Äù) and **online clustering** (‚ÄúGardener‚Äù).

---

## 3. Implementation Roadmap

### 3.1 Phase 1: MVP ‚Äî System Prototype

**Goal:** Implement the physical pipeline of **host-memory KV mounting** on a **4√ó MI100 + ~1TB host RAM** node, and verify the RoPE realignment scheme.

#### 3.1.1 KV Management Operators

- Implement CUDA kernels:
  - `derotate_kv(k, v, cos, sin)`  
  - `rerotate_kv(k, v, new_pos_ids)`

- Implement PCIe asynchronous transfer:
  - Use `torch.cuda.Stream` to perform **non-blocking CPU ‚Üí GPU copies**.

#### 3.1.2 Simple Indexer

- Temporarily **do not** train Q-Former.  
- Use **mean pooling** over each block as the key for indexing.  
- **Trigger policy:**
  - Every 512 tokens generated, forcibly retrieve the **3 most recent blocks** and splice their KV back in.

#### 3.1.3 Validation Metrics

- **Perplexity (PPL) Test:**  
  After inserting historical blocks, check whether **next-token perplexity** significantly decreases.

- **Needle In A Haystack:**  
  Long-context retrieval test to validate **RoPE phase alignment** correctness.

---

### 3.2 Token-Level Memory Management: Summary Trigger & Trimmer

At the **block level**, the system must decide:

1. **When** to cut off the current stream and archive a block (summary trigger).  
2. **Within that block**, **which tokens‚Äô KV** remain on GPU and which can be safely evicted to host (Trimmer).

This section defines the **token-level mechanisms** that sit *inside* each block:

- A **learned summary trigger** (`<SUMMARIZE>`) that decides when a block should be closed and archived.  
- A **Trimmer module** that learns, under a fixed KV budget, **which tokens are worth keeping** for future computation.

> Q-Former, block-level summaries, and block handlers are covered later in **Section 3.3**.

---

#### 3.2.1 Summary Trigger & Block Segmentation

The **summary trigger** is the junction between the **continuous token stream** and the **block-based memory system**.

We introduce a special control token:

- `<SUMMARIZE>` ‚Äì a learned ‚Äúsegment boundary‚Äù marker.

When `<SUMMARIZE>` fires at time step `t`:

1. The **current block** `[t_block_start, ..., t]` is finalized.  
2. The block‚Äôs hidden states and KV become eligible for:
   - Trimming (token-level KV selection).  
   - Archival (block-level semantic compression, see Section 3.3).  
3. The Memory OS:
   - Updates anchors (Global / Local).  
   - Starts a **new block** from `t+1`.

The summary trigger is learned but backed by simple heuristics to guarantee robustness.

---

##### 3.2.1.1 Supervised Summary Trigger

We provide **explicit supervision** for where `<SUMMARIZE>` *should* appear.

**Data construction**

For each long sequence (dialogue, document, code), we generate **segment boundaries** by:

- Human / heuristic / LLM-based topic segmentation:
  - Discourse boundaries (end of an answer, end of a paragraph).  
  - Topic shifts (‚Äúnow let‚Äôs talk about X‚Äù).  
  - Completion of a logical unit (e.g., a function definition or a resolved QA pair).

We then insert synthetic labels:

- At each boundary position `t`, the target next token is `<SUMMARIZE>`.  
- At non-boundary positions, `<SUMMARIZE>` is **not** in the target set.

**Training**

- The base LLM (optionally with a small LoRA head) is trained with **token-level CE** to predict `<SUMMARIZE>` at the correct boundaries.  
- To avoid over-firing:
  - We down-weight positive `<SUMMARIZE>` examples, or  
  - Add a simple **frequency penalty** in the loss.

At inference time, the model learns a **probability field over ‚Äúshould I summarize here?‚Äù**; the Memory OS then applies additional constraints.

---

##### 3.2.1.2 Heuristic Fallbacks & Safety Rails

To avoid pathological behaviors (never summarizing / summarizing every few tokens), the Memory OS adds simple, **non-learned fallback rules**:

- **Max block length**  
  - If the current block exceeds `L_max` tokens and no `<SUMMARIZE>` fired,  
    - force a segment cut.

- **Min block length**  
  - If `<SUMMARIZE>` fired too soon and the block is shorter than `L_min`,  
    - ignore new `<SUMMARIZE>` triggers for a short cooldown window.

- **Turn-based triggers (dialogue)**  
  - In conversational settings, when a **user turn** ends and the block is reasonably long,  
    - we can treat it as a soft segmentation hint.

These rules guarantee that the system **always** produces reasonably sized blocks, even if the learned trigger is imperfect.

---

#### 3.2.2 Trimmer: Learning Which Tokens‚Äô KV to Keep

Once a block is cut, we must decide **which tokens‚Äô KV states remain on GPU** and which can be safely evicted to host.

Instead of:

- Keeping a naive sliding window, or  
- Only preserving the first 4 tokens as anchors,

we train a **Trimmer module**:

> A **soft Top-K KV selector** that, given the block‚Äôs hidden states, learns which tokens are truly important under a fixed KV budget.

**Inputs**

- Block hidden states:  
  \[
  H = [h_1, h_2, \dots, h_T], \quad h_t \in \mathbb{R}^{d_\text{model}}
  \]
- An optional **summary condition**:
  - Either the hidden state at `<SUMMARIZE>` position,  
  - Or a pooled representation of the block:
    \[
    h_\text{sum} = \text{Pool}(H)
    \]

**Output**

- A scalar **importance logit** `l_t` for each token.  
- During training, these logits generate **soft masks**.  
- During inference, they are converted into **hard Top-K selections**.

---

##### 3.2.2.1 Scoring Network

The Trimmer is implemented as a lightweight scorer network:

\[
\tilde{h}_t = \text{LayerNorm}(h_t \oplus h_\text{sum}) \\
l_t = w^\top \sigma(W \tilde{h}_t + b) + c
\]

where:

- `‚äï` is concatenation or a simple fusion (e.g., gated addition).  
- `œÉ` is a non-linear activation (e.g., GELU / SiLU).  
- `l_t` is a scalar logit per token.

In matrix form, for block length `T`:

- `L_trim = [l_1, ..., l_T] ‚àà ‚Ñù^T`.

---

##### 3.2.2.2 Hard Gumbel Top-K (Fixed Budget)

In the **first version**, the KV budget is **fixed by heuristic**, not learned:

- We set a rule-based `K = K(T)`:
  - e.g., `K = K0 + Œ± ¬∑ log(T)` or a simple piecewise constant schedule.  
  - Typical choice:
    - Always keep:
      - global anchors  
      - local topic anchors
    - plus a small quota of **learned important tokens** from the Trimmer.

The selection mechanism:

- **Training time:**
  - Add Gumbel noise to `L_trim`:
    \[
    \hat{l}_t = l_t + g_t
    \]
  - Take **hard Top-K** according to `\hat{l}_t`:
    \[
    M^\text{hard}_t = 
    \begin{cases}
      1, & \text{if } t \text{ in Top-K}(\hat{l}) \\
      0, & \text{otherwise}
    \end{cases}
    \]
  - Use Straight-Through to define a soft version `M^soft` for backprop.

- **Inference time:**
  - No randomness:
    - Directly pick Top-K tokens from `L_trim`.  
  - Only those K tokens‚Äô KV remain on GPU; others are candidates for eviction / tombstones.

> Later work can relax this to **learning K** (via a sparsity penalty / budget regularizer).  
> Phase 1 deliberately uses a **simple, rule-based K** to keep the system stable.

---

#### 3.2.3 Teacher‚ÄìStudent Training: Adversarial Compression Under a Fixed Budget

The Trimmer is trained in a **teacher‚Äìstudent distillation** setup with a built-in **adversarial tension**:

- The **Teacher (SFT LLM)** wants to use **as much context as possible** to reduce loss.  
- The **Trimmer** is forced to operate under a **small K** (aggressive compression),  
  and must learn to pick **only the most necessary tokens**.

We do not explicitly optimize a game between two agents, but the training regime is **implicitly adversarial**:

- Data is constructed to **punish lazy trimming** (dropping important tokens).  
- The KV budget `K` is set small enough that the Trimmer *must* learn non-trivial selection.

---

##### 3.2.3.1 Training Setup

For each training example:

- There is a block `B` with tokens `[1..T]`.  
- We choose a generation position inside `B` or shortly after it.  
- We run:

1. **Teacher run (full KV)**
   - LLM runs on full context (no trimming).  
   - Produces:
     - logits `Y_teacher` for next token(s)  
     - optional attention maps `A_teacher`.

2. **Student run (trimmed KV)**
   - Apply Trimmer‚Äôs hard mask `M^hard` (Top-K) to KV inside block `B`:
     - `M_t = 0` ‚Üí KV for token `t` is masked / dropped.  
     - `M_t = 1` ‚Üí KV retained.  
   - LLM continues from same position with trimmed KV.  
   - Produces:
     - logits `Y_student`  
     - attention `A_student` (optional)

**Loss per mask:**

\[
\mathcal{L}_\text{trim} =
\alpha \cdot \mathcal{L}_\text{CE}(Y_{student}, Y_{target})
+ \beta \cdot \mathcal{L}_\text{KL}(Y_{student} \parallel Y_{teacher})
+ \gamma \cdot \mathcal{L}_\text{Attn-KL}(A_{student} \parallel A_{teacher})
\]

- `CE` ensures the trimmed model still hits the correct tokens.  
- `KL` ensures its **logit distribution** stays close to the full-context teacher.  
- `Attn-KL` (optional) propagates a soft prior from teacher attention.

Because `K` is small, any mask that discards essential tokens will incur **high CE/KL** ‚Üí gradients force the Trimmer to raise scores on such tokens.

---

##### 3.2.3.2 Sample Construction: Trimming Scenarios (Especially Topic Switches)

To make the Trimmer actually learn **semantically meaningful selection**, we need **stress-test scenarios** in training, especially around **topic changes**.

We deliberately construct at least three families of trimming scenarios:

1. **Intra-topic redundancy trimming (easy mode)**  
   - Long stretches of filler text (e.g., verbose explanations, small talk),  
   - Only a few tokens truly affect the next prediction (definitions, entities, key steps).  
   - Goal:
     - Trimmer learns to **drop redundant filler** while keeping structural anchors.

2. **Cross-topic boundaries (topic switch) ‚Äî ‚Äúforget the right past‚Äù**  
   - Training sequences are composed of:
     - **Older topic A**
     - then a clear switch to **topic B**, with its own internal definitions.
   - The target tokens belong purely to topic B.
   - Under full KV:
     - Teacher may still attend into topic A occasionally (long-range attention tails).  
   - Under trimming:
     - Trimmer is encouraged to:
       - Keep B‚Äôs **local anchors** and recent relevant tokens.  
       - Aggressively drop most of A, except:
         - global anchors  
         - any **bridging tokens** reused in B (e.g., shared entity, reused variable).

   - Data hint:
     - We can tag timestamps / topic IDs in metadata, and construct situations where:
       - A naive ‚Äúkeep all history‚Äù policy is wasteful,  
       - The optimal strategy is to **mostly forget previous topics**, but **not entirely**.

3. **Adversarial long-range dependence (hard mode)**  
   - Sequences with multiple ‚Äúchapters‚Äù:
     - A1, A2, B1, B2, ...,  
     - where the final question depends on **A1 + B2**, not the middle noise.  
   - Construct answers / next tokens that:
     - Are impossible without reading specific tokens in **older segments**.  
   - Here:
     - Teacher uses full KV and gets low loss.  
     - Any Trimmer that drops those few key tokens gets **punished**.

In all these scenarios:

- The **Teacher** is free to use all tokens.  
- The **Trimmer** is ‚Äúattacked‚Äù by the data:  
  - sequences are constructed so that **only very specific tokens** matter,  
  - but the KV budget `K` is kept small,  
  - so the Trimmer must learn to identify **semantic / structural pivot tokens** rather than just ‚Äúrecent ones‚Äù.

This realizes the ‚Äúadversarial‚Äù intuition:

> - Trimmer is forced to **keep as few tokens as possible** (small K, hard Top-K).  
> - The SFT / teacher setup **tries to expose as many useful dependencies as possible**, so dropping the wrong tokens is heavily penalized.

---

##### 3.2.3.3 Logical vs Physical Trimming

- **During training:**  
  - Trimming is **logical**:
    - We mask / zero out certain KV entries inside GPU memory.  
    - We do not move KV across PCIe.  
  - This keeps the training loop simple and differentiable.

- **During inference:**  
  - Trimming becomes **physical**:
    - Tokens selected by the Trimmer remain as live KV on GPU.  
    - Remaining tokens:
      - Either have their KV evicted to host,  
      - Or are discarded, leaving behind **tombstones** for future block-level retrieval.

This separation allows us to optimize purely for **information selection**, independently of systems details.

---

#### 3.2.4 Joint Adaptation with LoRA (Optional)

Empirically, large LLMs can be **brittle** under aggressive KV trimming:

- Some layers are highly sensitive to losing certain tokens.  
- Naive trimming can cause quality collapse in long-tail cases.

To mitigate this, we optionally:

- Attach a small **LoRA** to the base LLM.  
- Train **Trimmer and LoRA jointly**:

  - Trimmer learns to propose good masks under the KV budget.  
  - LoRA lets the LLM adapt to the new operating regime:
    - ‚ÄúKV is no longer dense; it has been filtered.‚Äù  
    - ‚ÄúPrefix KV may be injected from summaries / retrieved blocks.‚Äù

**Training schedule**

1. **Stage 1 ‚Äì Frozen LLM, train only Trimmer**
   - On top of a simple baseline (first-4 anchors + rolling window).  
   - Get a stable Trimmer that works under a rule-based `K`.

2. **Stage 2 ‚Äì Unfreeze LoRA, joint training**
   - LoRA helps recover performance lost due to trimming,  
   - Especially on:
     - very long contexts  
     - topic-switch cases  
     - adversarial long-range dependencies.

---

#### 3.2.5 Inference-Time Behavior (Within a Block)

For each block, the token-level pipeline is:

1. **Online generation**
   - LLM generates tokens while the Memory OS monitors:
     - `<SUMMARIZE>` predictions  
     - anchor policies  
   - KV accumulates until:
     - `<SUMMARIZE>` fires, or  
     - block length reaches `L_max`.

2. **Block finalization & archival handshake**
   - Once a block is closed:
     - The Memory OS first takes a snapshot of the **full block hidden states / KV (pre-trim)** and hands this snapshot to the block-level writer (Q-Former, Section 3.3) for archival.
     - Only after the snapshot is taken, the Trimmer scores all tokens in `[t_block_start, ..., t_block_end]`.  
     - Top-K tokens (anchors + high-score tokens) are marked ‚Äúto keep‚Äù as live KV on GPU.  
     - Others:
       - are evicted to host,  
       - or dropped, leaving behind **tombstones** that mark where information was archived.

3. **Hand-off to block-level memory (pre-trim snapshot)**
   - Section 3.3 always operates on this **pre-trim snapshot** of the block.  
   - Trimming only affects which KV stay **live** on GPU; it does **not** change what gets written into long-term block memory.

In summary:

- **3.2** defines a **write-side token-level game**:
  - A learned summary trigger decides **where to cut**.  
  - A Trimmer, under a fixed `K`, learns **which tokens deserve to survive**.  
- Block-level compression and retrieval, built on top of these trimmed blocks, are the topic of **Section 3.3**.

---

### 3.3 Core Architecture: Interpretation-Driven Dual-Phase Encoding (Block-Level)

After Section 3.2 has decided **when to cut a block** and **which tokens‚Äô KV remain on GPU**, we move to the **block-level write path**:

1. **First**, we train a **Q-Former-based writer** to compress each finalized 4K block into a set of **interpretation-friendly Detail Tokens** `D`, with:
   - Explanation SFT (logic-preserving)
   - Self-supervised reconstruction (detail-preserving)
2. **Only after write-side semantics are fixed**, we train a **two-tower indexer** that:
   - Maps `D` ‚Üí block handler / index embedding `H`
   - Maps LLM hidden state ‚Üí query embedding `Q`
   - Aligns `Q` and `H` via contrastive learning

This ordering is intentional:

> **All retrieval geometry lives on top of the Q-Former‚Äôs semantic bottleneck.  
> The indexer never sees raw blocks, only the frozen, explanation-aware `D`.  
> This structurally rules out a whole class of shortcuts.**

---

#### 3.3.1 Phase 1 (Write-Side): Semantic Compressor with Explanation & Reconstruction

**Role:** Semantic backbone of the block-level memory writer.  
**Goal:** Train a Q-Former to compress a 4K block into `N = 32` Detail Tokens `D` that:

- Are rich enough to reconstruct the block (no ‚Äúfingerprint loss‚Äù)  
- Are structured enough to support ‚Äúwhy retrieve / why not‚Äù style explanations  
- Are fully independent of the later indexer geometry

We use **two heads** and a carefully designed **data curriculum**.

> Input blocks at this stage are **already segmented by 3.2** (summary trigger).
> The Memory OS always takes a snapshot of the **full block hidden states / KV, before any Trimmer eviction**, and the Q-Former operates on this pre-trim snapshot.
> During training we may still simulate logical trimming for the teacher‚Äìstudent setup, but the archival snapshot `B` is always defined as the **pre-trim block**.

---

##### 3.3.1.1 Data Construction: Positive / Negative Pairs & Shortcut Control

Each training sample is a tuple:

\[
(\text{Block } B,\ \text{Query } q,\ \text{Label } y,\ \text{Metadata})
\]

where:

- `B`: a 4K token block (dialogue window, document span, code chunk, etc.)  
- `q`: a query that may or may not require information inside `B`  
- `y`: label indicating:
  - `y = 1`: **B is a supporting block** for `q`  
  - `y = 0`: `B` is **not** sufficient / relevant for `q`  
- `Metadata`: structured description (e.g., version, entity IDs, timestamps) used only for **data generation**, not exposed as input

**Key constraint** (sameÁ≤æÁ•û as 3.2, but now at block level):

We actively design data so that the model **cannot** solve the task by:

- Using only the query `q` and its prior knowledge  
- Memorizing global patterns like ‚Äúv2.0 questions almost always use later blocks‚Äù  
- Ignoring `D` and still answering correctly

Concretely:

1. **Inject block-local facts that are not in the base LLM**
   - Synthetic IDs, random hashes, opaque codes  
   - Local redefinitions: e.g., rebind `var_x` within `B`  
   - Custom ‚Äúv1.0 vs v2.0‚Äù semantics that contradict public knowledge / defaults

2. **Construct hard positives**
   - `y = 1` samples where:
     - Without `B`, the query is **unanswerable**  
     - With `B`, it becomes answerable via a small number of reasoning steps  
   - This forces the model to use `D` as the **only reliable source** for certain key facts.

3. **Construct hard negatives (two types)**
   - **Type N1 ‚Äì Semantic-but-wrong:**
     - Block `B_neg` has **similar topic / entities** as the true block,  
       but is logically incompatible:
       - Different version  
       - Different parameter  
       - Same function name but different behavior  
     - For these, the correct rationale is:
       > ‚ÄúI do **not** retrieve this because ‚Ä¶‚Äù
   - **Type N2 ‚Äì Distractor overlaps:**
     - Blocks that share some superficial patterns (names, keywords)  
       but are entirely irrelevant to the specific query.

By construction, any model that **ignores `D`** and relies on `q` priors will:

- Do well on easy cases, but  
- Be systematically wrong on these **adversarially shaped negatives**.

This creates a strong incentive to encode **block-local, non-global** information into `D`.

---

##### 3.3.1.2 Head A: Explanation SFT (Logic-Driven Head)

Given `(B, q, y)`:

1. Encode the block with Q-Former to get `D`:

   \[
   D = \text{QFormer}(B)
   \]

2. Feed the frozen LLM with:
   - The query `q`  
   - A special memory segment containing `D`, bracketed by:
     - `<MEM_START>` ‚Ä¶ `<MEM_END>`

3. Train the LLM (with a small trainable head or LoRA) to generate a **Retrieval Rationale**:

   - If `y = 1` (positive block):

     > ‚ÄúI retrieve this because it defines `var_x` used in question (2).  
     > It also introduces the v2.0 behavior that the query is asking about.‚Äù

   - If `y = 0` (negative block):

     > ‚ÄúI do **not** retrieve this because this block describes v1.0  
     > while the question asks specifically about v2.0 semantics.‚Äù

The loss `\(\mathcal{L}_\text{rationale}\)` can be:

- Pure sequence-level CE over rationale tokens, or  
- A mix of:
  - CE (for fluency)  
  - Classification head (retrieve / not retrieve)  
  - Span-level supervision (pointing to specific facts in `B`)

**Why this avoids shortcuts:**

- Hard negatives are constructed so that:
  - Top-level topic and entities can be similar, but  
  - Correctness hinges on **fine details in `B`**.  
- Any rationale that does **not** read those details from `D` will be inconsistent with label `y`.

---

##### 3.3.1.3 Head B: Self-Supervised Reconstruction (Coverage & Fingerprints)

Explanation alone is not enough.  
We also need `D` to carry **raw, non-verbalizable fingerprints**:

- hashes  
- opaque identifiers  
- arbitrary numeric patterns  
- local structural quirks

We add a **self-supervised reconstruction head**:

1. The model sees:
   - (Optionally) the query `q`  
   - Detail Tokens `D` as a prefix memory segment

2. It is trained to reconstruct a target derived from `B`, e.g.:

   - Full token sequence of `B` (causal LM loss), or  
   - A compressed representation:
     - Important spans  
     - Extracted key-value pairs  
     - Chunked tokens with span corruption / denoising

3. Loss `\(\mathcal{L}_\text{recon}\)`:

   - Token-level CE (standard LM loss)  
   - Optionally combined with a span-level objective (e.g., denoising autoencoder)

**Effect:**

- `\(\mathcal{L}_\text{recon}\)` is:
  - Self-supervised (no extra labels)  
  - A cheap ‚Äúinsurance policy‚Äù ensuring that `D` is **information-complete**  
- To minimize `\(\mathcal{L}_\text{recon}\)`, the Q-Former must encode:
  - IDs  
  - Rare tokens  
  - Local structure  
  that may **never** be mentioned in natural-language rationales.

---

##### 3.3.1.4 Phase 1 Objective & Freezing

The Phase 1 loss:

\[
\mathcal{L}_\text{Phase1} = \mathcal{L}_\text{rationale} + \lambda_\text{recon} \cdot \mathcal{L}_\text{recon}
\]

After convergence:

- Q-Former is **frozen**.  
- `D` is now:
  - Explanation-aware (via SFT)  
  - Detail-complete (via reconstruction)  
  - Independent of any particular retrieval index geometry

These Detail Tokens `D` become the canonical **Tier-2 payload** stored in **L2 Semantic Payload** (see Section 2.3).

---

#### 3.3.2 Phase 2 (Write-Side ‚Üí Index): Block Handler via Two-Tower Distillation

**Role:** Learn a **retrieval-friendly geometry** on top of the frozen semantic space `D`.  
**Goal:** Convert each block‚Äôs `D` into a single **index embedding** `H`, and align it with LLM query embeddings `Q`.

Important constraints:

- The **writer tower** sees only `D` (Phase 1 output).  
  - It does **not** see raw block text or KV.  
  - This structurally blocks a large class of shortcuts.

- The **reader tower** lives on the LLM side (read-side), but it learns purely from:
  - LLM hidden states as input,  
  - Block-level supervision (which block should be retrieved),  
  - **No direct access** to block contents.

---

##### 3.3.2.1 Writer Tower: `D ‚Üí H` (Block Handler)

- **Input:** frozen `D` for a block `B`  
- **Architecture:**
  - Tiny Transformer (2‚Äì4 layers) over the sequence of Detail Tokens  
  - Random token dropout (e.g., mask 30‚Äì50% of `D` during training) to:
    - Prevent over-reliance on a single ‚Äúgolden‚Äù token  
    - Encourage **global use** of the semantic space

- **Output:**

  \[
  H = g(D) \in \mathbb{R}^d
  \]

  a single **index embedding** per block.

`H` is what we store in **L1 Hot Index** (e.g., HNSW / DNII).

---

##### 3.3.2.2 Reader Tower: `h_seed ‚Üí Q` (LLM Query Head)

- **Input:** a **seed hidden state** `h_\text{seed}` from the LLM:

  - At a `<RETRIEVE>` token, or  
  - Constructed by fusing:
    - current position hidden  
    - relevant tombstone hidden states  
    - local context summary

- **Architecture:**
  - MLP / Mixer / small gated network

- **Output:**

  \[
  Q = f(h_\text{seed}) \in \mathbb{R}^d
  \]

`Q` lives in the **same embedding space** as `H`.

---

##### 3.3.2.3 Contrastive Training: Aligning `Q` and `H`

For each training instance, we know:

- The **supporting blocks** `B_\text{pos}` whose Detail Tokens `D_\text{pos}` contain the answer  
- A pool of negatives `B_\text{neg}`:
  - Random negatives  
  - Hard negatives:
    - same topic, wrong version  
    - same entity, wrong time  
    - same function name, different behavior

We compute:

\[
H_\text{pos} = g(D_\text{pos}), \quad
H_\text{neg} = g(D_\text{neg}), \quad
Q = f(h_\text{seed})
\]

Then use an **InfoNCE loss**:

\[
\mathcal{L}_\text{index} = - \log \frac{\exp(\text{sim}(Q, H_\text{pos}) / \tau)}
{\exp(\text{sim}(Q, H_\text{pos}) / \tau) + \sum_{H \in \mathcal{N}} \exp(\text{sim}(Q, H) / \tau)}
\]

where:

- `sim` is cosine similarity or dot product  
- `ùí©` contains in-batch negatives + mined hard negatives

This teaches:

- `Q` to land near the **correct block handlers**  
- `H` to be **discriminative** among semantically similar but logically different blocks

After Phase 2:

- `D` (Phase 1) remains frozen, stored in **L2**  
- `H` (Phase 2) forms the **L1 index** for fast ANN / DNII retrieval  
- The reader tower `f` becomes the **retrieval head** for the LLM.

---

#### 3.3.3 Read-Side (Runtime): Using `H` and `D` at Inference

With write-side semantics fixed, runtime retrieval decomposes into:

1. **When to retrieve?** ‚Äì decided by the Memory OS and `<RETRIEVE>` / tombstone mechanisms (see Section 3.2 & policy section).  
2. **How to build `Q`?** ‚Äì apply the reader tower `f` to the chosen `h_\text{seed}`.  
3. **How to reuse blocks?** ‚Äì use `H` for fast lookup, `D` for semantic injection.

At a high level:

1. **Trigger**  
   - Model emits `<RETRIEVE>`, or  
   - Tombstone attention / periodic policy decides to retrieve.

2. **Query projection**  
   - Construct `h_\text{seed}` from current / tombstone / local summary hidden states.  
   - Compute:
     \[
     Q = f(h_\text{seed})
     \]

3. **Index search (L1)**  
   - Use `Q` to query the L1 Hot Index over `{H}` (HNSW / DNII).  
   - Obtain Top-K block IDs.

4. **Load semantic payload (L2)**  
   - For each retrieved block:
     - Load its frozen Detail Tokens `D` from L2 Semantic Payload (host RAM).

5. **Inject `D` as soft prompt**  
   - Insert retrieved `D` into the context, bracketed by `<MEM_START>` / `<MEM_END>`.  
   - For a short horizon `T_\text{protect}`, protect these tokens from trimming so the model can fully use them.

6. **Optional raw KV expansion (L3)**  
   - In rare, explicit cases (verbatim quoting, oracle-style analysis),  
     - Load raw, de-rotated KV from L3 Archive,  
     - Re-rotate with RoPE,  
     - Splice into the GPU KV cache.

---

**Summary for 3.3**

- **Phase 1 (Q-Former):**  
  - Learns an **interpretation-friendly, detail-complete** semantic representation `D` for each block, using:
    - Explanation SFT (logic)  
    - Self-supervised reconstruction (coverage / fingerprints)  

- **Phase 2 (Two-Tower Indexer):**  
  - Learns a **retrieval geometry** (`H`, `Q`) *on top of `D`*, without touching raw blocks.  

- **Runtime:**  
  - Memory OS decides **when** to retrieve (`<RETRIEVE>` + tombstones).  
  - Reader tower maps LLM hidden state to `Q`.  
  - Index returns block handlers `H`; we fetch `D` and inject them as the **default recall payload**, with raw KV as an optional fallback.

---

### 3.4 Phase 4: Production ‚Äî System Optimization

**Goal:** After the algorithmic pipeline is working, optimize inference speed to **production-grade** (> 20 tokens/s).

#### 3.4.1 Storage Quantization

- **Payload:**  
  - Host-side KV cache is compressed to **FP8 (E4M3 / E5M2)** to reduce PCIe bandwidth pressure.

- **Kernel:**  
  - Custom FP8 ‚Üí BF16 dequantization kernels to decompress KV **on GPU at load time**.

#### 3.4.2 Prefetch Pipeline

- Use Python multithreading or `torch.cuda.Stream` to overlap compute and I/O:

  - While the model computes **Layer `N`**, asynchronously prefetch **Layer `N+1`**‚Äôs KV blocks.
  - Combined with the graph-based Walker:
    - Predict likely **next hops** in the Memory Graph.
    - Prefetch corresponding Detail Tokens `D` and/or KV blocks.

#### 3.4.3 Hierarchical / Tree Index Optimization

- Build a coarse **Time ‚Üí Tag tree index** on top of the graph.  
- Use LLM-generated tags for **L1 hard pruning**, shrinking the Walker‚Äôs initial search space from ~1M blocks to ~10K-level candidates.

---

## 4. Risk Analysis

| Risk                          | Severity | Mitigation                                                              |
|------------------------------|----------|-------------------------------------------------------------------------|
| RoPE phase misalignment      | High     | Must store **unrotated KV**; recompute rotation on insertion.          |
| Trimmer gradient vanishing   | High     | Use **Gumbel exploration + NTP loss**; Gumbel keeps exploration alive; NTP & KL provide signal. |
| PCIe bandwidth bottlenecks   | Medium   | Limit number of recalled blocks per step (Top-K ‚â§ 5); use FP8; pinned memory. |
| Read‚Äìwrite semantic mismatch | Medium   | Asymmetric training: write-side autoencoding (objective), read-side adaptation (subjective). |

---

## 5. Appendix: Q-Former Pseudo Code

```python
class MemoryCompressor(nn.Module):
    def __init__(self, d_model, num_summary_tokens=32):
        super().__init__()
        self.latents = nn.Parameter(
            torch.randn(1, num_summary_tokens, d_model)
        )
        self.encoder = TransformerEncoder(...) 
        # Using SwiGLU for better projection
        self.decoder_proj = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.SiLU(),
            nn.Linear(d_model * 4, d_model),
        )

    def forward(self, block_embeds):
        # 1. Compress 2048/4096 tokens -> 32 latents
        summary_embeds = self.encoder(
            query=self.latents, 
            context=block_embeds
        )
        # 2. Project back into LLM space (used for both
        #    reconstruction and as Detail Tokens D)
        projected_summary = self.decoder_proj(summary_embeds)
        return summary_embeds, projected_summary
