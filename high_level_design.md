# High-Performance Infinite-Context Inference System  
**Design Doc: Host-Resident Memory & Sparse Semantic Retrieval**

---

## 1. Executive Summary

This project aims to build an **‚Äúinfinite-context‚Äù inference system** that supports long-term conversational memory of **1M+ tokens** on a **single consumer GPU** (e.g., RTX 3090/4090) combined with **large-capacity host RAM (64GB+)**.

The core innovation is to **decouple Compute and Memory**:

- **Storage layer:** Use host RAM to store massive KV caches, removing the GPU VRAM bottleneck.  
- **Indexing layer:** Introduce a **Q-Former** for semantic compression and vector indexing.  
- **Compute layer:** Use **RoPE dynamic re-phasing** to enable seamless **retrieve-and-inject** context stitching.

---

## 2. System Architecture

The system adopts a layered architecture with **separation of storage and compute** (read/write separation).

### 2.1 Memory OS Controller (Control Layer)

This is the ‚Äúbrain‚Äù of the system, residing inside the LLM inference loop. It intercepts special tokens and orchestrates low-level resources.

**Responsibilities:**

- **State machine management:** Switch between  
  - `Generating` (during token generation)  
  - `Archiving` (during memory write/compaction)  
  - `Retrieving` (during memory recall)
- **Signal interception:** Listen for  
  - `<SUMMARIZE>` (triggers archiving)  
  - `<RETRIEVE>` (triggers retrieval)
- **VRAM management:** Handle KV cache **splitting (Split)**, **eviction/migration (Evict)**, and **concatenation (Concat)**.

#### Anchor-Aware Eviction Policy

To address the semantic loss caused by simple sliding windows in StreamingLLM, we introduce a **three-tier VRAM residency strategy**:

1. **Global Sinks (physical anchors)**  
   - **Keep:** Positions `0‚Äì4` (System Prompt + BOS).

2. **Local Anchors (semantic anchors)**  
   - **Keep:** When the **Segmentation Policy** detects a new topic, lock the first **K tokens** of that topic.

3. **Rolling Buffer (working memory)**  
   - **Keep:** The most recently generated **M tokens**.

4. **Tombstones (memory tombstones)**  
   - **Keep:** For each evicted token span, retain **one residual token** generated by the Q-Former.  
   - This acts as a placeholder for ‚Äúforgotten information‚Äù and helps guide retrieval decisions.

---

### 2.2 Semantic Indexing Engine (Semantic Layer)

Responsible for transforming unstructured KV blocks into searchable vectors, and mapping query intent into the same vector space.

- **Write-side (Compressor):**  
  - Based on **Q-Former**.  
  - Compress large KV blocks (e.g., **4096 tokens**) into a compact set of **summary embeddings** (e.g., **32 vectors**).

- **Read-side (Projector):**  
  - Based on a **Gated MLP**.  
  - Map LLM hidden states into the **summary embedding space**.

---

### 2.3 Hierarchical Storage (Storage Layer)

Uses a **hot‚Äìcold separation** strategy to balance speed and capacity.

- **L1 Hot Index (VRAM / RAM):**  
  - Stores summary embeddings generated by the Q-Former.  
  - Supports high-frequency vector retrieval.

- **L2 Cold Payload (Host RAM / SSD):**  
  - Stores **de-rotated (unphased)** raw KV cache in FP8 format.  
  - Loaded via PCIe only when retrieval hits.

---

### 2.4 Cognitive Memory Graph (Topology Layer)

**Structure:** A dynamically maintained **graph** that acts as a higher-level form of the L1 Hot Index.

- **Nodes:**
  - **Real Nodes:** Summaries of concrete memory blocks.  
  - **Virtual Nodes:** Dynamically created cluster centers (topic centroids).

- **Edges:**
  - **Temporal:** Next/Prev relations along time.  
  - **Hierarchical:** Belonging relations (Real Node ‚Üí Virtual Node).

---

## 3. Implementation Roadmap

### 3.1 Phase 1: MVP ‚Äî System Prototype

**Goal:** Implement the physical pipeline of **‚ÄúHost Memory KV mounting‚Äù** and verify the RoPE realignment scheme.

#### 3.1.1 KV Management Operators

- Implement CUDA kernels:  
  - `derotate_kv(k, v, cos, sin)`  
  - `rerotate_kv(k, v, new_pos_ids)`
- Implement **PCIe asynchronous transfer:**  
  - Use `torch.cuda.Stream` to perform non-blocking **CPU ‚Üí GPU** copies.

#### 3.1.2 Simple Indexer

- Temporarily **do not train Q-Former**.  
- Use **mean pooling** over each block as the key for indexing.
- **Trigger policy:**  
  - Every **512 tokens** generated, forcibly retrieve the **3 most recent blocks**.

#### 3.1.3 Validation Metrics

- **PPL Test:**  
  - After inserting historical blocks, check whether **next-token perplexity** significantly decreases.
- **Needle In A Haystack:**  
  - Long-context retrieval test to validate **phase alignment correctness**.

---

### 3.2 Core Architecture: Interpretation-Driven Dual-Phase Encoding

This system adopts an aggressive **‚Äúinterpret first, distill later‚Äù** dual-phase architecture.  
We do **not** train retrieval vectors directly. Instead, we first train a *reasoning-aware compressor*, then distill its outputs into compact retrieval indices.

---

#### 3.2.1 Phase 1: The Semantic Compressor (The Cornerstone)

**Role:** The semantic backbone of the system.  
**Goal:** Use the LLM‚Äôs causal reasoning ability as a **regularizer**, forcing a Q-Former to compress a 4K-token block into a set of **reasoning-aware Detail Tokens** (Tier-2 payload).  
**Key design:** Prevent information loss and shortcut learning during compression.

**Model structure**

- **Writer:** Q-Former (BERT/ViT-style) + MLP  
- **Output:** `N = 32` **Detail Tokens** (`D`)  
  - These tokens are **not** just textual summaries.  
  - They are **carriers of logical information**.

**Training strategy: Explanation SFT**

- **Task:**  
  Given a `(Block, Query)` pair:
  - Encode the block with the Q-Former to obtain Detail Tokens `D`.
  - Inject `D` as a **soft prompt** into a **frozen LLM**.
  - Ask the LLM to generate a **Retrieval Rationale** (why this block should or should not be retrieved).

- **Data curriculum (crucial):**  
  At least **50% hard negatives**, so the Q-Former is forced to encode *discriminative* features.

  - **Type A (Positive):**  
    > ‚ÄúI retrieve this because it defines `var_x` needed by the query.‚Äù  
    ‚Üí Forces encoding of semantic associations.

  - **Type B (Negative ‚Äî The Anti-Shortcut):**  
    > ‚ÄúI do **not** retrieve this because the query asks for v2.0 features,  
    > but this block describes v1.0.‚Äù  

- **Intuition:**  
  For the LLM to generate a *rejection* rationale, the Detail Tokens `D` must clearly preserve the **v1.0 vs v2.0** distinction, not just a fuzzy ‚Äúversion-ish‚Äù hint.  
  This is a **physical mechanism** to prevent feature collapse and shortcut solutions.

- **Control tokens:**  
  Jointly train special tokens:
  - `<MEM_START>`
  - `<MEM_END>`  
  so that the model learns **semantic isolation** of the memory segment.

- **Result:**  
  After Phase 1 training:
  - The Q-Former is **frozen**.  
  - The resulting Detail Tokens `D` form a **high-fidelity, causally-informed semantic intermediate representation**.

---

#### 3.2.2 Phase 2: The Index Distiller (Alignment & Indexing)

**Role:** The retrieval adapter of the system.  
**Goal:** On top of the **frozen** Phase-1 outputs, train a lightweight distiller that resolves the tension between **semantic completeness** and **retrieval geometry**, producing the Tier-1 index vectors.

**Model structure: Asymmetric two-tower**

- **Writer tower (The Distiller):**
  - **Input:** Frozen Detail Tokens `D` from Phase 1
  - **Architecture:** Tiny Transformer (2‚Äì4 layers)
  - **Trick ‚Äì Random Token Dropout (input masking):**
    - During training, randomly mask out **30‚Äì50%** of the input tokens.
    - **Intuition:** Forces the Tiny Transformer to scan **global** information instead of overfitting to a single ‚Äúgolden‚Äù token.
  - **Output:** A single **Index Embedding** `H`.

- **Reader tower (Query Projector):**
  - **Input:** Hidden state of the main LLM (e.g., at the retrieval step)
  - **Architecture:** MLP / Mixer
  - **Output:** A single **Query Embedding** `Q`.

**Training strategy: Contrastive alignment**

- **Task:**  
  Map the **explanatory semantic space** (Detail Tokens `D`) into a **metric retrieval space**.

- **Loss:**  
  - **InfoNCE** (listwise contrastive loss).  
  - **No BCE loss**: we explicitly avoid binary classification; we want a **well-structured embedding topology**, not just ‚Äúrelevant / non-relevant‚Äù logits.

- **Hard negative mining:**  
  - In each batch, mix in blocks that are:
    - **Semantically very similar**  
    - But **logically contradictory** (e.g., different IDs, different versions).  
  - These are mined using Phase-1 signals.  
  - This forces the Distiller to pick up the **sharp features** preserved in Phase 1 (IDs, version numbers, etc.), not just topic-level similarity.

---

#### 3.2.3 Inference Workflow (Runtime)

At inference time, the system reuses the products of both phases to decouple **fast retrieval** from **deep understanding**.

**Write path (indexing)**

1. `Block ->(Frozen Q-Former)-> Detail Tokens (D) ->(Save)-> Host RAM (Tier 2)`
2. `Detail Tokens (D) ->(Tiny Transformer)-> Index Embedding (H) ->(Save)-> HNSW Index (Tier 1)`

- Phase 1 guarantees the **quality and faithfulness** of `D`.  
- Phase 2 guarantees the **retrieval directionality** of `H`.

**Read path (retrieval)**

1. `LLM Context ->(Projector)-> Query (Q)`
2. **Search:**  
   `Q` vs HNSW over `H` ‚Üí Top-K hits
3. **Inject:**  
   Load the corresponding Detail Tokens `D` from Host RAM and inject them as a **soft prompt** into the current LLM context.
4. **Generate:**  
   The LLM generates the final answer, conditioned on these **reasoning-aware Detail Tokens**.


---

### 3.3 Phase 3: Algorithm II ‚Äî Cognitive Memory Graph

**Goal:** Build a dynamic graph based on **virtual nodes** (topic-level abstractions).

#### 3.3.1 System Diagram

```mermaid
graph TD
    %% Style Definitions
    classDef storage fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef algo fill:#f0f4c3,stroke:#827717,stroke-width:2px,rx:5,ry:5;
    classDef agent fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,rx:10,ry:10;
    classDef virtual fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,stroke-dasharray: 5 5;

    subgraph Memory_Graph ["üß† Cognitive Memory Graph"]
        direction TB
        VN1((Virtual Node 1<br/>Topic: Coding)):::virtual
        VN2((Virtual Node 2<br/>Topic: Life)):::virtual
        
        B1(Block 1) <==>|Temporal| B2(Block 2)
        VN1 -.->|Belongs To| B1
        VN1 -.->|Belongs To| B2
        VN2 -.->|Belongs To| B50
        
        note1[Self-Organized Topology<br/>No RL in Construction]
        style note1 fill:#fff,stroke:#333
    end

    subgraph Macro_Write ["The Gardener (Online Clustering)"]
        New_Block[New Block Summary] --> Cluster_Algo
        Cluster_Algo[Streaming K-Means / BIRCH]:::algo --> Update_Graph
        Update_Graph -->|1. Assign to VN| Memory_Graph
        Update_Graph -->|2. Drift VN Center| Memory_Graph
        Update_Graph -->|3. Split/Merge VN| Memory_Graph
    end
    
    subgraph Macro_Read ["The Walker (RL Agent)"]
        Query[User Query] --> Policy_Net_W
        Current_Node[Current Node] --> Policy_Net_W
        Policy_Net_W[MLP Policy] --> Step{Hop/Stop?}
        Memory_Graph -->|Neighbors| Policy_Net_W
    end
