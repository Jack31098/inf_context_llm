# High-Performance Infinite-Context Inference System  
**Design Doc: Host-Resident Memory & Sparse Semantic Retrieval**

---

## 1. Executive Summary

This project aims to build an **‚Äúinfinite-context‚Äù inference system** that supports long-term conversational memory of **1M+ tokens** on a **single consumer GPU** (e.g., RTX 3090/4090) combined with **large-capacity host RAM (64GB+)**.

The core innovation is to **decouple Compute and Memory**:

- **Storage layer:** Use host RAM to store massive KV caches, removing the GPU VRAM bottleneck.  
- **Indexing layer:** Introduce a **Q-Former** for semantic compression and vector indexing.  
- **Compute layer:** Use **RoPE dynamic re-phasing** to enable seamless **retrieve-and-inject** context stitching.

---

## 2. System Architecture

The system adopts a layered architecture with **separation of storage and compute** (read/write separation).

### 2.1 Memory OS Controller (Control Layer)

This is the ‚Äúbrain‚Äù of the system, residing inside the LLM inference loop. It intercepts special tokens and orchestrates low-level resources.

**Responsibilities:**

- **State machine management:** Switch between  
  - `Generating` (during token generation)  
  - `Archiving` (during memory write/compaction)  
  - `Retrieving` (during memory recall)
- **Signal interception:** Listen for  
  - `<SUMMARIZE>` (triggers archiving)  
  - `<RETRIEVE>` (triggers retrieval)
- **VRAM management:** Handle KV cache **splitting (Split)**, **eviction/migration (Evict)**, and **concatenation (Concat)**.

#### Anchor-Aware Eviction Policy

To address the semantic loss caused by simple sliding windows in StreamingLLM, we introduce a **three-tier VRAM residency strategy**:

1. **Global Sinks (physical anchors)**  
   - **Keep:** Positions `0‚Äì4` (System Prompt + BOS).

2. **Local Anchors (semantic anchors)**  
   - **Keep:** When the **Segmentation Policy** detects a new topic, lock the first **K tokens** of that topic.

3. **Rolling Buffer (working memory)**  
   - **Keep:** The most recently generated **M tokens**.

4. **Tombstones (memory tombstones)**  
   - **Keep:** For each evicted token span, retain **one residual token** generated by the Q-Former.  
   - This acts as a placeholder for ‚Äúforgotten information‚Äù and helps guide retrieval decisions.

---

### 2.2 Semantic Indexing Engine (Semantic Layer)

Responsible for transforming unstructured KV blocks into searchable vectors, and mapping query intent into the same vector space.

- **Write-side (Compressor):**  
  - Based on **Q-Former**.  
  - Compress large KV blocks (e.g., **4096 tokens**) into a compact set of **summary embeddings** (e.g., **32 vectors**).

- **Read-side (Projector):**  
  - Based on a **Gated MLP**.  
  - Map LLM hidden states into the **summary embedding space**.

---

### 2.3 Hierarchical Storage (Storage Layer)

Uses a **hot‚Äìcold separation** strategy to balance speed and capacity.

- **L1 Hot Index (VRAM / RAM):**  
  - Stores summary embeddings generated by the Q-Former.  
  - Supports high-frequency vector retrieval.

- **L2 Cold Payload (Host RAM / SSD):**  
  - Stores **de-rotated (unphased)** raw KV cache in FP8 format.  
  - Loaded via PCIe only when retrieval hits.

---

### 2.4 Cognitive Memory Graph (Topology Layer)

**Structure:** A dynamically maintained **graph** that acts as a higher-level form of the L1 Hot Index.

- **Nodes:**
  - **Real Nodes:** Summaries of concrete memory blocks.  
  - **Virtual Nodes:** Dynamically created cluster centers (topic centroids).

- **Edges:**
  - **Temporal:** Next/Prev relations along time.  
  - **Hierarchical:** Belonging relations (Real Node ‚Üí Virtual Node).

---

## 3. Implementation Roadmap

### 3.1 Phase 1: MVP ‚Äî System Prototype

**Goal:** Implement the physical pipeline of **‚ÄúHost Memory KV mounting‚Äù** and verify the RoPE realignment scheme.

#### 3.1.1 KV Management Operators

- Implement CUDA kernels:  
  - `derotate_kv(k, v, cos, sin)`  
  - `rerotate_kv(k, v, new_pos_ids)`
- Implement **PCIe asynchronous transfer:**  
  - Use `torch.cuda.Stream` to perform non-blocking **CPU ‚Üí GPU** copies.

#### 3.1.2 Simple Indexer

- Temporarily **do not train Q-Former**.  
- Use **mean pooling** over each block as the key for indexing.
- **Trigger policy:**  
  - Every **512 tokens** generated, forcibly retrieve the **3 most recent blocks**.

#### 3.1.3 Validation Metrics

- **PPL Test:**  
  - After inserting historical blocks, check whether **next-token perplexity** significantly decreases.
- **Needle In A Haystack:**  
  - Long-context retrieval test to validate **phase alignment correctness**.

---

### 3.2 Phase 2: Algorithm I ‚Äî Semantic Indexing Core

**Goal:** Train the **Q-Former** and **Projector** so the system can **understand history** and **retrieve memory accurately**.

---

#### 3.2.1 Segmentation & Trimming Policy

**Strategy A: Context Trimmer with Stochastic Exploration**  
(based on Gumbel exploration)

Introduce a **Straight-Through Gumbel-Softmax (STE)** mechanism, and optimize the trimmer by minimizing the generation error introduced by trimming.

- **Trimmer model:**  
  A lightweight scorer:  
  `LayerNorm -> Linear -> Linear`  
  Outputs raw logits \( L_{\text{trim}} \).

##### Training (Soft-to-Hard Distillation)

1. **Sampling:**  
   - Add Gumbel noise to \( L_{\text{trim}} \).  
   - Perform multi-sample Top-K sampling to obtain **N different hard masks** \( M_{\text{hard}} \).

2. **Parallel Forward (Simulation):**  
   - Use the **N** variants of \( M_{\text{hard}} \) to simulate trimmed KV caches  
     (i.e., apply masking / zero-filling to the original KV).  
   - During training, **no physical eviction** is performed.

3. **Loss Function (End-to-End):**

   - **Teacher output:** \( Y_{\text{teacher}} \) (logits from full context)  
   - **Student output:** \( Y_{\text{student}} \) (logits from trimmed context using \( M_{\text{hard}} \))

   \[
   \mathcal{L}_{\text{trim}} = 
   \alpha \cdot \mathcal{L}_{\text{CE}}(Y_{\text{student}}, Y_{\text{target}})
   + \beta \cdot \mathcal{L}_{\text{Logit-KL}}(Y_{\text{student}} \parallel Y_{\text{teacher}})
   + \gamma \cdot \mathcal{L}_{\text{Attn-KL}}(\text{Trimmer Logits} \parallel \text{Teacher Attn})
   \]

   - **Mechanism:**  
     - Gumbel noise provides exploration.  
     - The loss penalizes the deviation in logits caused by trimming.

##### Inference (Physical Trim)

- Only at **inference time**, the trimmer outputs logits and we:
  - Apply **Top-K** selection.  
  - **Physically zero out and evict** KV entries that are not selected, migrating them to host RAM.

---

#### 3.2.2 Write-Side: Compression & Alignment

This pipeline uses an **integrated training** strategy, operating directly in the **text domain** to avoid maintaining a massive KV dataset.

##### On-the-fly Context Perturbation

- **Input:** Raw text corpus.  
- **Process:** During the training loop, apply masking or trimming strategies to the text in real time (simulating inference-time forgetting).  
- **Forward:** Feed the processed text into a **frozen LLM**.  
- **Output:** On-the-fly hidden states with **contextual noise**.

**Advantage:** Completely avoids the version alignment and storage overhead of offline KV dumps.

##### Latent Generation (Compression)

- **Input:** Block-level hidden states generated on the fly.  
- **Module:** Trainable **Q-Former**.  
- **Operation:** Cross-attention.  
- **Output:** **Summary embeddings**.

##### Semantic Reconstruction (Alignment)

- **Goal:** Force the summary embeddings to retain sufficient semantic information.
- **Task:**  
  - Use the summary embeddings as a **soft prompt prefix**.  
  - Ask the LLM to reconstruct the original block text (Causal LM Loss).

---

#### 3.2.3 Read-Side: Retrieval Projection

Executed when the model needs to **retrieve** memory.

##### 1. Retrieval Policy: Tombstone Activation

- **Mechanism:** During inference, monitor the attention weights to **tombstone tokens** in the context.  
- **Trigger:**  
  - If attention > threshold, the model is likely needing the compressed information behind that tombstone.  
- **Action:**  
  - Trigger `<RETRIEVE>` and use this tombstone as a **key** to retrieve the original block.

##### 2. Retrieval Matching: Learning What to Ask

- **Module:** **Gated MLP (SwiGLU) Projector**.  
- **Input:**  
  - Final-layer hidden state \( h_{\text{ret}} \) when the LLM generates the `<RETRIEVE>` token.  
- **Output:**  
  - Predicted **summary embedding**.  
- **Loss:**  
  - **InfoNCE (contrastive loss)**.

---

### 3.3 Phase 3: Algorithm II ‚Äî Cognitive Memory Graph

**Goal:** Build a dynamic graph based on **virtual nodes** (topic-level abstractions).

#### 3.3.1 System Diagram

```mermaid
graph TD
    %% Style Definitions
    classDef storage fill:#e1f5fe,stroke:#01579b,stroke-width:2px;
    classDef algo fill:#f0f4c3,stroke:#827717,stroke-width:2px,rx:5,ry:5;
    classDef agent fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px,rx:10,ry:10;
    classDef virtual fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,stroke-dasharray: 5 5;

    subgraph Memory_Graph ["üß† Cognitive Memory Graph"]
        direction TB
        VN1((Virtual Node 1<br/>Topic: Coding)):::virtual
        VN2((Virtual Node 2<br/>Topic: Life)):::virtual
        
        B1(Block 1) <==>|Temporal| B2(Block 2)
        VN1 -.->|Belongs To| B1
        VN1 -.->|Belongs To| B2
        VN2 -.->|Belongs To| B50
        
        note1[Self-Organized Topology<br/>No RL in Construction]
        style note1 fill:#fff,stroke:#333
    end

    subgraph Macro_Write ["The Gardener (Online Clustering)"]
        New_Block[New Block Summary] --> Cluster_Algo
        Cluster_Algo[Streaming K-Means / BIRCH]:::algo --> Update_Graph
        Update_Graph -->|1. Assign to VN| Memory_Graph
        Update_Graph -->|2. Drift VN Center| Memory_Graph
        Update_Graph -->|3. Split/Merge VN| Memory_Graph
    end
    
    subgraph Macro_Read ["The Walker (RL Agent)"]
        Query[User Query] --> Policy_Net_W
        Current_Node[Current Node] --> Policy_Net_W
        Policy_Net_W[MLP Policy] --> Step{Hop/Stop?}
        Memory_Graph -->|Neighbors| Policy_Net_W
    end
