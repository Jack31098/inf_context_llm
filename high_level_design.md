# High-Performance Infinite-Context Inference System

**Design Doc: Host-Resident Memory & Sparse Semantic Retrieval**

---

## 1. Executive Summary

This project aims to build an **“infinite-context” inference system** that supports **long-term conversational memory of 1M+ tokens** on a **single 4× MI100 node + large host RAM (~1024GB)**.

The core innovation is to decouple **Compute** and **Memory**:

- **Storage layer:** Use host RAM to store massive KV caches, removing the GPU HBM/VRAM bottleneck.  
- **Indexing layer:** Introduce a Q-Former for semantic compression and vector indexing.  
- **Compute layer:** Use RoPE dynamic re-phasing to enable seamless retrieve-and-inject context stitching.

The end goal is a **memory OS** for mid-sized LLMs (e.g., Qwen3-8B) that can:

- Decide **what to remember / what to forget** at the **token level**  
- Organize history into **semantic memory blocks**  
- Retrieve relevant blocks later via **differentiable neural retrieval**, not brute-force scanning

---

## 2. System Architecture

The system adopts a layered architecture with **separation of storage and compute** (read/write separation).

### 2.1 Memory OS Controller (Control Layer)

This is the **“brain”** of the system, residing inside the LLM inference loop. It intercepts special tokens and orchestrates lower-level resources.

**Responsibilities**

- **State machine management**  
  Switch between:
  - `Generating` – normal token generation  
  - `Archiving` – memory write / compaction  
  - `Retrieving` – memory recall and context augmentation  

- **Signal interception**  
  Listen for special control tokens:
  - `<SUMMARIZE>` – triggers archiving of recent context into memory blocks  
  - `<RETRIEVE>` – triggers block-level retrieval  

- **VRAM/HBM management**  
  Handle KV cache:
  - **Split** – partition KV into manageable blocks  
  - **Evict** – migrate old KV to host or archive  
  - **Concat** – stitch KV segments (including recalled blocks) into the current context

#### Anchor-Aware Eviction Policy

To address the semantic loss caused by simple sliding windows in StreamingLLM, we introduce a **three-tier GPU-residency strategy**:

1. **Global Sinks (physical anchors)**  
   - **Keep:** Positions `0–4` (System prompt + BOS)  
   - These tokens serve as the **absolute reference frame** for the model.

2. **Local Anchors (semantic anchors)**  
   - **Keep:** When the Segmentation Policy detects a **new topic**, lock the first `K` tokens of that topic.  
   - These act as **semantic “chapter headers”**.

3. **Rolling Buffer (working memory)**  
   - **Keep:** The most recently generated `M` tokens.  
   - Provides short-term working context for local reasoning.

4. **Tombstones (memory tombstones)**  
   - **Keep:** For each evicted token span, retain a **single residual token** generated by the Q-Former.  
   - This acts as a placeholder for “forgotten information” and helps guide **retrieval decisions** (e.g., via attention to tombstones).

---

### 2.2 Semantic Indexing Engine (Semantic Layer)

Responsible for transforming unstructured KV blocks into **searchable vectors**, and mapping **query intent** into the same vector space.

- **Write-side (Compressor):**
  - Based on a **Q-Former**.
  - Compress large KV blocks (e.g., 4096 tokens) into a compact set of **summary / Detail embeddings** (e.g., 32 vectors).

- **Read-side (Projector):**
  - Based on a **Gated MLP**.
  - Map LLM hidden states (e.g., at `<RETRIEVE>` positions) into the **summary embedding space**.

This provides a **bridge** between the LLM’s internal representation and the external memory index.

---

### 2.3 Hierarchical Storage (Storage Layer)

The system uses a **multi-tier storage hierarchy** to balance speed, capacity, and semantic richness across **4× MI100 HBM** and **~1TB host RAM**.

We explicitly distinguish between:

- **What we retrieve by default** (semantic, compact, interpretable)  
- **What we keep as an optional archive** (full raw KV, lossless but heavy)

#### L1: Hot Index (GPU HBM / Host RAM)

- **Stores:**
  - **Index embeddings** `H` (from the Index Distiller in Phase 2)  
  - **Centroids / routing structures** (for DNII / HNSW)

- **Role:**
  - Supports **high-frequency vector retrieval** given a query embedding `Q`.  
  - Lives partially on GPU, partially in pinned host memory for fast access.

#### L2: Semantic Payload (Host RAM — Default Recall Path)

- **Stores:**
  - **Detail Tokens** `D` produced by the frozen Q-Former (Phase 1) for each block.

- **Role:**
  - This is the **primary payload** for memory recall.  
  - At retrieval time, the system **loads `D` and injects them as a soft prompt** into the LLM.  
  - `D` are:
    - Compact (e.g., 32 tokens per 4K block)  
    - **Reasoning-aware** (trained via Explanation SFT + Reconstruction)  
    - Suitable for **semantic, interpretable recall**.

#### L3: Raw KV Archive (Host RAM / SSD — Optional Fallback)

- **Stores (optional):**
  - **De-rotated (unphased) raw KV cache** in FP8 format for each archived block.

- **Role:**
  - Acts as an **optional, lossless backup** for cases where:
    - Verbatim quoting is required (“show me the exact original text”)  
    - Extremely fine-grained reconstruction is needed beyond what `D` can capture.
  - Loaded via PCIe **only on demand**, and spliced back into GPU KV via RoPE re-phasing.

- **Default behavior:**
  - **Not used in the normal retrieval path.**  
  - The default infinite-context workflow uses **L2 Detail Tokens** only, to stay consistent with the **interpretation-driven architecture**.

---

### 2.4 Cognitive Memory Graph (Topology Layer)

**Structure:** A dynamically maintained graph that acts as a **higher-level structure** over L1/L2.

- **Nodes:**
  - **Real Nodes:** Summaries of concrete memory blocks (e.g., block-level Detail Tokens or their centroids).  
  - **Virtual Nodes:** Dynamically created **cluster centers** (topic centroids, timelines, entities).

- **Edges:**
  - **Temporal:** `Next / Prev` relations along time.  
  - **Hierarchical:** Belonging relations (`Real Node → Virtual Node`).  

In later phases, this graph becomes the substrate for **macro-level navigation** (RL-based “Walker”) and **online clustering** (“Gardener”).

---

## 3. Implementation Roadmap

### 3.1 Phase 1: MVP — System Prototype

**Goal:** Implement the physical pipeline of **host-memory KV mounting** on a **4× MI100 + ~1TB host RAM** node, and verify the RoPE realignment scheme.

#### 3.1.1 KV Management Operators

- Implement CUDA kernels:
  - `derotate_kv(k, v, cos, sin)`  
  - `rerotate_kv(k, v, new_pos_ids)`

- Implement PCIe asynchronous transfer:
  - Use `torch.cuda.Stream` to perform **non-blocking CPU → GPU copies**.

#### 3.1.2 Simple Indexer

- Temporarily **do not** train Q-Former.  
- Use **mean pooling** over each block as the key for indexing.  
- **Trigger policy:**
  - Every 512 tokens generated, forcibly retrieve the **3 most recent blocks** and splice their KV back in.

#### 3.1.3 Validation Metrics

- **Perplexity (PPL) Test:**  
  After inserting historical blocks, check whether **next-token perplexity** significantly decreases.

- **Needle In A Haystack:**  
  Long-context retrieval test to validate **RoPE phase alignment** correctness.

---

### 3.2 Phase 2: Algorithm I — Semantic Indexing (Core)

**Goal:** Train the Trimmer, Q-Former, and retrieval projector so that the system can:

- **Read** historical context meaningfully,  
- **Compress** it into reasoning-aware Detail Tokens, and  
- **Retrieve** the right blocks with a learned query embedding.

---

#### 3.2.1 Segmentation & Trimming Policy (Token-Level)

**Strategy A: Context Trimmer with Stochastic Exploration**  
(*Gumbel-based smart pruning*)

We introduce a **Trimmer** to learn **which tokens’ KV should be kept** under a fixed budget.

- **Mechanism: Straight-Through Gumbel-Softmax (STE)**
  - Provide exploration via Gumbel noise.  
  - Optimize the Trimmer by minimizing the **generation error** induced by trimming.

- **Trimmer model:**
  - A lightweight **Scorer**:
    - `LayerNorm -> Linear -> Linear`  
  - Outputs raw logits `L_trim` over tokens.

- **Training: Soft-to-Hard Distillation**

  - **Sampling:**
    - Apply Gumbel noise to `L_trim` and perform **multi-sample Top-K** sampling.  
    - Obtain `N` different hard masks `M_hard`.

  - **Parallel Forward (Simulation):**
    - For each `M_hard`, simulate a **trimmed KV cache** (masking / zero-filling).  
    - During training, no physical eviction; only logical masking.

  - **Loss function (end-to-end):**
    - Teacher output: `Y_teacher` (logits from the full-context model)  
    - Student output: `Y_student` (logits from trimmed context)  

    \[
    \mathcal{L}_\text{trim} = \alpha \cdot \mathcal{L}_\text{CE}(Y_{student}, Y_{target}) +
    \beta \cdot \mathcal{L}_\text{Logit-KL}(Y_{student} \parallel Y_{teacher}) +
    \gamma \cdot \mathcal{L}_\text{Attn-KL}(\text{Trimmer Logits} \parallel \text{Teacher Attn})
    \]

    - Gumbel provides the exploration path.  
    - The loss penalizes logit deviations induced by trimming.

  - **Inference (physical trim):**
    - At inference time, the Trimmer outputs logits → Top-K selection.  
    - KV entries not selected are physically zeroed and evicted to host RAM.

---

#### 3.2.2 Write-Side: Interpretation-Driven Dual-Phase Encoding

We adopt an aggressive **“interpret first, distill later”** dual-phase architecture for block-level memory:

> We do **not** train retrieval vectors directly.  
> We first train a **reasoning-aware compressor** (Phase 1),  
> then distill its outputs into compact retrieval indices (Phase 2).

---

##### 3.2.2.1 Phase 1: The Semantic Compressor (The Cornerstone)

**Role:** Semantic backbone of the memory system.  
**Goal:** Use the LLM’s causal reasoning ability as a **regularizer**, forcing a Q-Former to compress a 4K-token block into a set of **reasoning-aware Detail Tokens** `D` (Tier-2 payload).  
**Key design:** Prevent both:
- semantic / logical information loss, and  
- **“fingerprint” loss** (tiny but crucial differences: IDs, hashes, version tags)

during compression.

**Model structure**

- **Writer:** Q-Former (BERT/ViT-style) + MLP  
- **Output:** `N = 32` **Detail Tokens** `D` per block  
  - Not just textual summaries  
  - They are **carriers of logical and fingerprint-level information**

---

###### Two-Headed Objective

Phase 1 combines **two complementary heads**:

1. **Head A — Explanation SFT (Logic-Driven Head)**  
2. **Head B — Reconstruction Head (Coverage & Fingerprints)**  

\[
\mathcal{L}_\text{Phase1} = \mathcal{L}_\text{rationale} + \lambda_\text{recon} \cdot \mathcal{L}_\text{recon}
\]

Where:

- `L_rationale` teaches **why** a block is relevant or not.  
- `L_recon` ensures `D` retains **all information needed** to reconstruct the original block, including non-verbalizable details.

---

###### Head A: Explanation SFT (Logic-Driven Head)

For each `(Block, Query)` pair:

1. Encode the block with the Q-Former to obtain Detail Tokens `D`.  
2. Inject `D` as a **soft prompt** into a **frozen LLM**.  
3. Ask the LLM to generate a **Retrieval Rationale**:

   - **Type A (Positive):**  
     > “I retrieve this because it defines `var_x` needed by the query.”  
     → Forces `D` to encode semantic associations and supporting facts.

   - **Type B (Negative — Anti-Shortcut):**  
     > “I do **not** retrieve this because the query asks for v2.0 features,  
     > but this block describes v1.0.”  
     → Forces `D` to encode fine-grained **exclusion signals** (wrong version, wrong ID, wrong condition).

**Curriculum (crucial)**

- At least **50% hard negatives**:
  - Blocks that are **semantically similar** but **logically incompatible** with the query.  
- To generate good *rejection* rationales, `D` must preserve **sharp distinctions** (e.g., v1.0 vs v2.0), not just fuzzy topic vectors.

---

###### Head B: Reconstruction Head (Coverage & Fingerprints)

Explanation alone is **not enough**:

- Some differences are hard to verbalize:
  - Hash-like IDs  
  - Opaque keys / codes  
  - Arbitrary numeric patterns  

If we rely only on Explanation SFT, the Q-Former may **“get away with” discarding** these details, as they rarely appear in natural-language rationales.

To prevent this, we add a **self-supervised reconstruction task**:

- **Setup:**
  - Use the original block text (or its token sequence) as target.  
  - Feed the LLM with:
    - (Optionally) the Query  
    - The Detail Tokens `D` as a **soft prefix**.  
  - Train a small **decoder head / LM head** (or reuse the frozen LLM head) to reconstruct the original block.

- **Loss:**
  - Reconstruction loss `L_recon`:
    - Causal LM loss over the original block tokens  
    - Or hybrid (token CE + span-level contrastive)

- **Intuition:**
  - `L_recon` is:
    - **Self-supervised** (no human labels)  
    - A zero-cost **“insurance policy”** against fingerprint loss.  
  - To minimize `L_recon`, `D` must contain **all information necessary** to reconstruct the original block, including:
    - IDs  
    - Rare tokens  
    - Local structures that may never appear in rationales.

---

###### Control Tokens

We jointly train special control tokens:

- `<MEM_START>`  
- `<MEM_END>`

so that the LLM learns **semantic isolation** of the memory segment injected via `D`:

- Inside `<MEM_START> ... <MEM_END>`:
  - The model focuses on using `D` for rationale + reconstruction.
- Outside:
  - Normal LLM behavior is preserved.

---

###### Phase 1 Result

After Phase 1:

- The Q-Former is **frozen**.  
- Detail Tokens `D` form a **high-fidelity, causally-informed, reconstruction-capable representation** of each block.  
- `D` are stored in **L2 Semantic Payload (host RAM)** as the **default recall unit**.

---

##### 3.2.2.2 Phase 2: The Index Distiller (Alignment & Indexing)

**Role:** Retrieval adapter of the system.  
**Goal:** On top of frozen `D`, train a **lightweight distiller** to resolve the tension between:

- **Semantic completeness** (kept in `D`) and  
- **Retrieval geometry** (embedding space for ANN)

and produce **Tier-1 index vectors** `H`.

**Asymmetric two-tower structure**

- **Writer tower (Index Distiller):**
  - **Input:** frozen Detail Tokens `D` from Phase 1  
  - **Architecture:** Tiny Transformer (2–4 layers)  
  - **Trick — Random Token Dropout (input masking):**
    - During training, randomly mask out **30–50%** of tokens in `D`.  
    - Forces the distiller to use **global information**, not overfit to a single “golden token”.  
  - **Output:** single **Index Embedding** `H` per block.

- **Reader tower (Query Projector):**
  - **Input:** main LLM hidden state (e.g., at a `<RETRIEVE>` token or retrieval step)  
  - **Architecture:** MLP / Mixer  
  - **Output:** single **Query Embedding** `Q`.

**Training – Contrastive alignment**

- **Task:** map the **interpretive space** (`D`) to a **metric retrieval space** (`H`, `Q`) suitable for ANN / DNII.
- **Loss:**
  - **InfoNCE** (listwise contrastive loss).  
  - **No BCE loss** – we want a **well-shaped embedding topology**, not just a “yes/no” classifier.

- **Hard negative mining:**
  - In each batch, mix in blocks that:
    - Are **semantically close** (same topic / entity)  
    - But **logically incompatible** with the query (different version, wrong ID, wrong condition)
  - Hard negatives are constructed using Phase 1 signals (rationales, tags).  
  - Forces the distiller to preserve the **sharp features** encoded in `D`.

**Phase 2 Result**

- `H` lives in a retrieval-friendly space (HNSW / DNII).  
- `D` remains a rich, interpretable representation in host RAM.  
- At runtime, retrieval is:
  - `Q → ANN over H → block IDs → load D for those blocks`.

---

#### 3.2.3 Read-Side: Retrieval Projection & Runtime Workflow

At inference time, the system reuses both phases to decouple:

- **Fast retrieval** (ANN over compact index embeddings `H`)  
- **Deep understanding** (LLM reasoning over Detail Tokens `D`)

The **default recall path** is **Detail-Token-based soft prompting**.  
Raw KV injection is treated as an **optional, advanced expansion mode**.

---

##### Write path (Indexing)

For each completed block:

1. **Semantic payload (L2):**  
   `Block  ->(Frozen Q-Former)->  Detail Tokens D  ->(Save)->  L2 Semantic Payload (Host RAM)`

2. **Index embeddings (L1):**  
   `Detail Tokens D  ->(Tiny Transformer Distiller)->  Index Embedding H  ->(Save)->  L1 Hot Index (HNSW / DNII)`

3. **Optional raw KV archive (L3):**  
   `Block KV  ->(De-rotate + FP8)->  Raw KV Archive  ->(Save)->  L3 (Host RAM / SSD)`  
   - Used only for **rare, verbatim, or full-context reconstruction** cases.

- Phase 1 guarantees the **quality & faithfulness** of `D`.  
- Phase 2 guarantees the **retrieval geometry** of `H`.  
- L3, if enabled, guarantees an **ultimate fallback** for lossless recovery.

---

##### Read path (Retrieval)

1. **Query embedding:**
   - From the current LLM context (e.g., at a `<RETRIEVE>` token or a scheduled retrieval step):

     `LLM Context  ->(Query Projector)->  Query embedding Q`

2. **Index search (L1):**
   - Use `Q` to query the Hot Index:

     `Q  vs  HNSW / DNII over {H}  →  Top-K block IDs`

3. **Default injection (L2 – Detail Tokens):**
   - Load the corresponding Detail Tokens `D` from L2 (host RAM).  
   - Inject `D` into the current context as a **soft prompt / memory segment**, bracketed by:
     - `<MEM_START>`  
     - `<MEM_END>`  
   - This is the **default, interpretation-driven recall mechanism**.

4. **Generation:**
   - The LLM continues generation, conditioned on:
     - Current local KV (recent context)  
     - Injected Detail Tokens `D` (long-term memory)

---

##### Optional: Raw KV expansion (L3 – Fallback Mode)

In rare cases, the system (or user) may require **verbatim or full-structure reconstruction**:

- Triggered by:
  - A special control token, e.g., `<EXPAND_RAW>`  
  - Or an explicit user request: “quote the original paragraph exactly”

- Behavior:
  1. For selected Top-K blocks, load their **de-rotated FP8 raw KV** from L3.  
  2. Re-rotate via RoPE to the appropriate position IDs.  
  3. Splice the recovered KV into the GPU KV cache.

- Trade-off:
  - Much higher GPU memory & bandwidth cost.  
  - Only used selectively, not in the default infinite-context workflow.

---

### 3.3 Phase 3: Algorithm II — Cognitive Memory Graph

**Goal:** Build a dynamic graph over memory blocks using **Virtual Nodes**, to support macro-level navigation.

#### 3.3.1 System Diagram (Conceptual)

(Original mermaid-style diagram omitted here; see repo for full graph illustration.)

Core ideas:

- **Memory Graph**
  - Virtual Nodes (topics)  
  - Real Nodes (memory blocks)  
  - Temporal & hierarchical edges  

- **Macro-Write (“The Gardener”)**
  - Online clustering of blocks into Virtual Nodes  
  - Self-organizing, unsupervised topology

- **Macro-Read (“The Walker”)**
  - RL agent navigating the graph:
    - Macro jumps between Virtual Nodes  
    - Micro steps within a node’s blocks

---

#### 3.3.2 Macro-Write: The Gardener (Self-Organizing)

**Core idea:** Graph construction is **objective, unsupervised**. It reflects the **density and distribution** of memories in semantic space.

- **Role:** Online clustering algorithm (non-neural agent).  
- **Mechanism:**
  - **Assign:** When a new block summary `S_new` appears, compute its distance to existing Virtual Nodes (centroids).  
  - **Update:** Move the nearest centroid toward `S_new` (e.g., EMA). This simulates topic drift over time.  
  - **Spawn:** If `S_new` is far from all existing centroids (distance > threshold), create a new Virtual Node.  
  - **Split:** If a Virtual Node has high variance among its attached blocks, split it into two new Virtual Nodes.

---

#### 3.3.3 Macro-Read: The Walker (RL-Based Navigator)

**Core idea:** Graph *usage* is subjective. The Walker navigates the Gardener-maintained “highway network”.

- **Role:** RL agent (policy network).  
- **Action space:**
  - **Macro-Jump:** Move to a neighboring Virtual Node (topic switch).  
  - **Micro-Step:** Within the current Virtual Node, select specific blocks (detail lookup).

- **Training:**
  - Only train the Walker.  
  - Environment (graph construction) is fixed → relatively stable RL environment → fast convergence.

---

### 3.4 Phase 4: Production — System Optimization

**Goal:** After the algorithmic pipeline is working, optimize inference speed to **production-grade** (> 20 tokens/s).

#### 3.4.1 Storage Quantization

- **Payload:**  
  - Host-side KV cache is compressed to **FP8 (E4M3 / E5M2)** to reduce PCIe bandwidth pressure.

- **Kernel:**  
  - Custom FP8 → BF16 dequantization kernels to decompress KV **on GPU at load time**.

#### 3.4.2 Prefetch Pipeline

- Use Python multithreading or `torch.cuda.Stream` to overlap compute and I/O:

  - While the model computes **Layer `N`**, asynchronously prefetch **Layer `N+1`**’s KV blocks.
  - Combined with the graph-based Walker:
    - Predict likely **next hops** in the Memory Graph.
    - Prefetch corresponding Detail Tokens `D` and/or KV blocks.

#### 3.4.3 Hierarchical / Tree Index Optimization

- Build a coarse **Time → Tag tree index** on top of the graph.  
- Use LLM-generated tags for **L1 hard pruning**, shrinking the Walker’s initial search space from ~1M blocks to ~10K-level candidates.

---

## 4. Risk Analysis

| Risk                          | Severity | Mitigation                                                              |
|------------------------------|----------|-------------------------------------------------------------------------|
| RoPE phase misalignment      | High     | Must store **unrotated KV**; recompute rotation on insertion.          |
| Trimmer gradient vanishing   | High     | Use **Gumbel exploration + NTP loss**; Gumbel keeps exploration alive; NTP & KL provide signal. |
| PCIe bandwidth bottlenecks   | Medium   | Limit number of recalled blocks per step (Top-K ≤ 5); use FP8; pinned memory. |
| Read–write semantic mismatch | Medium   | Asymmetric training: write-side autoencoding (objective), read-side adaptation (subjective). |

---

## 5. Appendix: Q-Former Pseudo Code

```python
class MemoryCompressor(nn.Module):
    def __init__(self, d_model, num_summary_tokens=32):
        super().__init__()
        self.latents = nn.Parameter(
            torch.randn(1, num_summary_tokens, d_model)
        )
        self.encoder = TransformerEncoder(...) 
        # Using SwiGLU for better projection
        self.decoder_proj = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.SiLU(),
            nn.Linear(d_model * 4, d_model),
        )

    def forward(self, block_embeds):
        # 1. Compress 2048/4096 tokens -> 32 latents
        summary_embeds = self.encoder(
            query=self.latents, 
            context=block_embeds
        )
        # 2. Project back into LLM space (used for both
        #    reconstruction and as Detail Tokens D)
        projected_summary = self.decoder_proj(summary_embeds)
        return summary_embeds, projected_summary
