### 3.2 Core Architecture: Interpretation-Driven Dual-Phase Encoding

The core of this system is **write-first**:

1. **First,** we train a **Q-Former-based writer** to compress each 4K block into a set of **interpretation-friendly Detail Tokens** `D`, with:
   - Explanation SFT (logic-preserving)
   - Self-supervised reconstruction (detail-preserving)
2. **Only after write-side semantics are fixed,** we train a **two-tower indexer** that:
   - Maps `D` ‚Üí block handler / index embedding `H`
   - Maps LLM hidden state ‚Üí query embedding `Q`
   - Aligns `Q` and `H` via contrastive learning

This ordering is intentional:

> **All retrieval geometry lives on top of the Q-Former‚Äôs semantic bottleneck.  
> The indexer never sees raw blocks, only the frozen, explanation-aware `D`.  
> This structurally rules out a whole class of shortcuts.**

---

#### 3.2.1 Phase 1 (Write-Side): Semantic Compressor with Explanation & Reconstruction

**Role:** Semantic backbone of the memory writer.  
**Goal:** Train a Q-Former to compress a 4K block into `N = 32` Detail Tokens `D` that:

- Are rich enough to reconstruct the block (no ‚Äúfingerprint loss‚Äù)  
- Are structured enough to support ‚Äúwhy retrieve / why not‚Äù style explanations  
- Are fully independent of the later indexer geometry

We use **two heads** and a carefully designed **data curriculum**.

---

##### 3.2.1.1 Data construction: Positive / Negative pairs & shortcut control

Each training sample is a tuple:

\[
(\text{Block } B,\ \text{Query } q,\ \text{Label } y,\ \text{Metadata})
\]

where:

- `B`: a 4K token block (dialogue window, document span, code chunk, etc.)  
- `q`: a query that may or may not require information inside `B`  
- `y`: label indicating:
  - `y = 1`: **B is a supporting block** for q  
  - `y = 0`: B is **not** sufficient / relevant for q  
- `Metadata`: structured description (e.g., version, entity IDs, timestamps) used only for **data generation**, not exposed as input

**Key constraint:**  
We actively design data so that the model **cannot** solve the task by:

- Using only the query `q` and its prior knowledge  
- Memorizing global patterns like ‚Äúv2.0 questions almost always use later blocks‚Äù  
- Ignoring `D` and still answering correctly

Concretely:

1. **Inject block-local facts that are not in the base LLM**  
   - Synthetic IDs, random hashes, opaque codes  
   - Local redefinitions: e.g., rebind `var_x` within `B`  
   - Custom ‚Äúv1.0 vs v2.0‚Äù semantics that contradict public knowledge

2. **Construct hard positives:**
   - `y = 1` samples where:
     - Without `B`, the query is **unanswerable**  
     - With `B`, it becomes answerable via a small number of reasoning steps  
   - This forces the model to use `D` as the **only source** for certain key facts.

3. **Construct hard negatives (two types):**
   - **Type N1 ‚Äì Semantic-but-wrong:**
     - Block `B_neg` has **similar topic / entities** as the true block,  
       but is logically incompatible:
       - Different version  
       - Different parameter  
       - Same function name but different behavior  
     - For these, the correct rationale is:
       > ‚ÄúI do **not** retrieve this because ‚Ä¶‚Äù
   - **Type N2 ‚Äì Distractor overlaps:**
     - Blocks that share some superficial patterns (names, keywords)  
       but are entirely irrelevant to the specific query.

By design, any model that **ignores `D`** and relies on `q` priors will:

- Do well on easy cases  
- But be systematically wrong on these constructed hard negatives

This creates a strong incentive to encode **block-local, non-global** information into `D`.

---

##### 3.2.1.2 Head A: Explanation SFT (Logic-Driven Head)

Given `(B, q, y)`:

1. Encode the block with Q-Former to get `D`:
   \[
   D = \text{QFormer}(B)
   \]

2. Feed the frozen LLM with:
   - The query `q`  
   - A special memory segment containing `D`, bracketed by:
     - `<MEM_START>` ‚Ä¶ `<MEM_END>`

3. Train the LLM (with a small trainable head or LoRA) to generate a **Retrieval Rationale**:

   - If `y = 1` (positive block):
     > ‚ÄúI retrieve this because it defines `var_x` used in question (2).  
     > It also introduces the v2.0 behavior that the query is asking about.‚Äù

   - If `y = 0` (negative block):
     > ‚ÄúI do **not** retrieve this because this block describes v1.0  
     > while the question asks specifically about v2.0 semantics.‚Äù

The loss `L_rationale` can be:

- Pure sequence-level CE over rationale tokens  
- Or a mix of:
  - CE (for fluency)  
  - Classification head (retrieve / not retrieve)  
  - Span-level supervision (pointing to specific facts in `B`)

**Why this avoids shortcuts:**

- Because hard negatives are constructed so that:
  - Top-level topic and entities can be similar  
  - But correctness hinges on fine details in `B`  
- Any rationale that does **not** read those details from `D` will be inconsistent with the label `y`.

---

##### 3.2.1.3 Head B: Self-Supervised Reconstruction (Coverage & Fingerprints)

Explanation alone is not enough.  
We also need `D` to carry **raw, non-verbalizable fingerprints**:

- hashes  
- opaque identifiers  
- arbitrary numeric patterns  
- local structural quirks

We add a **self-supervised reconstruction head**:

1. The model sees:
   - (Optionally) the query `q`  
   - Detail Tokens `D` as a prefix memory segment

2. It is trained to reconstruct a target derived from `B`, e.g.:

   - Full token sequence of `B` (causal LM loss)  
   - Or a compressed representation:
     - Important spans  
     - Extracted key-value pairs  
     - Chunked tokens with span corruption

3. Loss `L_recon`:

   - Token-level CE (standard LM loss)  
   - Optionally combined with a span-level objective (e.g., denoising autoencoder)

**Effect:**

- `L_recon` is:
  - Self-supervised (no extra labels)  
  - A cheap ‚Äúinsurance policy‚Äù ensuring that `D` is **information-complete**  
- To minimize `L_recon`, the Q-Former must encode:
  - IDs  
  - Rare tokens  
  - Local structure  
  that may **never** be mentioned in natural-language rationales.

---

##### 3.2.1.4 Phase 1 Objective & Freezing

The Phase 1 loss:

\[
\mathcal{L}_\text{Phase1} = \mathcal{L}_\text{rationale} + \lambda_\text{recon} \cdot \mathcal{L}_\text{recon}
\]

After convergence:

- Q-Former is **frozen**.  
- `D` is now:
  - Explanation-aware (via SFT)  
  - Detail-complete (via reconstruction)  
  - Independent of any particular retrieval index geometry

These Detail Tokens `D` become the canonical **Tier-2 payload** stored in L2.

---

#### 3.2.2 Phase 2 (Write-Side ‚Üí Index): Block Handler via Two-Tower Distillation

**Role:** Learn a **retrieval-friendly geometry** on top of the frozen semantic space `D`.  
**Goal:** Convert each block‚Äôs `D` into a single **index embedding** `H`, and align it with LLM query embeddings `Q`.

Important constraints:

- The **writer tower** sees only `D` (Phase 1 output).  
  - It does **not** see raw block text or KV.  
  - This structurally blocks a large class of shortcuts.

- The **reader tower** lives on the LLM side (read-side), but it learns purely from:
  - LLM hidden states as input  
  - Block-level supervision (which block should be retrieved),  
  - **No direct access** to block contents.

---

##### 3.2.2.1 Writer Tower: `D ‚Üí H` (Block Handler)

- Input: frozen `D` for a block `B`
- Architecture:
  - Tiny Transformer (2‚Äì4 layers) over the sequence of Detail Tokens
  - Random token dropout (e.g., mask 30‚Äì50% of `D` during training) to:
    - Prevent over-reliance on a single ‚Äúgolden‚Äù token  
    - Encourage **global use** of the semantic space

- Output:
  \[
  H = g(D) \in \mathbb{R}^d
  \]
  a single **index embedding** per block.

`H` is what we store in **L1 Hot Index** (e.g., HNSW / DNII).

---

##### 3.2.2.2 Reader Tower: `h_seed ‚Üí Q` (LLM Query Head)

- Input: a **seed hidden state** `h_seed` from the LLM:

  - At a `<RETRIEVE>` token  
  - Or constructed by fusing:
    - current position hidden  
    - relevant tombstone hidden states  
    - local context summary

- Architecture:
  - MLP / Mixer / small gated network

- Output:
  \[
  Q = f(h_\text{seed}) \in \mathbb{R}^d
  \]

`Q` lives in the **same embedding space** as `H`.

---

##### 3.2.2.3 Contrastive Training: Aligning `Q` and `H`

For each training instance, we know:

- The **supporting blocks** `B_pos` whose Detail Tokens `D_pos` contain the answer  
- A pool of negatives `B_neg`:
  - Random negatives  
  - Hard negatives:
    - same topic, wrong version  
    - same entity, wrong time  
    - same function name, different behavior

We compute:

\[
H_\text{pos} = g(D_\text{pos}), \quad
H_\text{neg} = g(D_\text{neg}), \quad
Q = f(h_\text{seed})
\]

Then use an **InfoNCE loss**:

\[
\mathcal{L}_\text{index} = - \log \frac{\exp(\text{sim}(Q, H_\text{pos}) / \tau)}
{\exp(\text{sim}(Q, H_\text{pos}) / \tau) + \sum_{H \in \mathcal{N}} \exp(\text{sim}(Q, H) / \tau)}
\]

where:

- `sim` is cosine similarity or dot product  
- `ùí©` contains in-batch negatives + mined hard negatives

This teaches:

- `Q` to land near the **correct block handlers**  
- `H` to be **discriminative** among semantically similar but logically different blocks

After Phase 2:

- `D` (Phase 1) remains frozen, stored in **L2**  
- `H` (Phase 2) forms the **L1 index** for fast ANN / DNII retrieval  
- The reader tower `f` becomes the **retrieval head** for the LLM

---

#### 3.2.3 Read-Side (Runtime): Retrieval Policy, Projection & Injection

With write-side semantics fixed, runtime retrieval is:

1. **When to retrieve?** (policy)  
2. **How to build `Q`?** (query construction)  
3. **How to reuse blocks?** (inject `D`, optionally expand KV)

Here we only sketch the core pieces; full DNII details are in Section 3.3.

---

##### 3.2.3.1 Retrieval Policy (When)

The Memory OS combines:

- **Model-driven triggers:**
  - LLM explicitly emits `<RETRIEVE>` at certain steps (supervised during training)  
  - High attention to **tombstone tokens** indicates the model is trying to access evicted content

- **System-driven safeguards:**
  - Periodic retrieval after long stretches without recall  
  - User / tool overrides:
    - `<FORCE_RETRIEVE>`  
    - `<DISABLE_RETRIEVE>`

In practice:

- **Primary route:** `<RETRIEVE>` + tombstone attention  
- **Fallback route:** periodic, budgeted retrieval to avoid ‚Äúnever retrieve‚Äù collapse

---

##### 3.2.3.2 Query Projection (What)

Given a retrieval trigger:

1. **Seed selection:**
   - `<RETRIEVE>`‚Äôs hidden state `h_ret`  
   - Or fusion of:
     - current hidden state  
     - one or more tombstone hidden states  
     - local summary

2. **Projection:**
   \[
   Q = f(h_\text{seed})
   \]
   using the **reader tower** trained in Phase 2.

`Q` is then sent into the **L1 Hot Index** (HNSW / DNII) to retrieve candidate blocks.

---

##### 3.2.3.3 Retrieval & Injection (How)

1. **Candidate retrieval (L1):**

   - Use `Q` to perform ANN / DNII search over `{H}`  
   - Obtain Top-K block IDs

2. **Load semantic payload (L2):**

   - For each block ID:
     - Load its frozen Detail Tokens `D` from L2 (host RAM)

3. **Default injection: Detail Tokens as soft prompt**

   - Inject retrieved `D` into current context:

     ```text
     ... [current context]
     <MEM_START>
     [D_block1 tokens ...]
     [D_block2 tokens ...]
     ...
     <MEM_END>
     ```

   - For the next `T_protect` steps, the Trimmer:
     - May evict older context  
     - But does **not** evict the newly injected `D` segment

4. **Optional raw KV expansion (L3):**

   - In rare cases (verbatim quoting, deep inspection), the system may:
     - Load raw KV from L3 for those blocks  
     - Re-rotate via RoPE  
     - Splice into the GPU KV cache

   - This path is **expensive** and is not used by default.

---

In summary:

- **Write-side (Phase 1 + 2):**
  - Q-Former learns an **interpretation-friendly, detail-complete** semantic space `D`  
  - Block handlers `H` live strictly on top of `D`, via a two-tower contrastive indexer

- **Read-side:**
  - LLM learns **when** to ask (`<RETRIEVE>` / tombstones)  
  - Query head turns its internal state into `Q` in the `H`-space  
  - Runtime uses `H` ‚Üí `D` ‚Üí soft prompting as the **primary recall path**, with raw KV as an optional fallback
