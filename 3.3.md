### 3.3 Core Architecture: Interpretation-Driven Dual-Phase Encoding (Block-Level)

After Section 3.2 has decided **when to cut a block** and **which tokens‚Äô KV remain on GPU**, we move to the **block-level write path**:

1. **First**, we train a **Q-Former-based writer** to compress each finalized 4K block into a set of **interpretation-friendly Detail Tokens** `D`, with:
   - Explanation SFT (logic-preserving)
   - Self-supervised reconstruction (detail-preserving)
2. **Only after write-side semantics are fixed**, we train a **two-tower indexer** that:
   - Maps `D` ‚Üí block handler / index embedding `H`
   - Maps LLM hidden state ‚Üí query embedding `Q`
   - Aligns `Q` and `H` via contrastive learning

This ordering is intentional:

> **All retrieval geometry lives on top of the Q-Former‚Äôs semantic bottleneck.  
> The indexer never sees raw blocks, only the frozen, explanation-aware `D`.  
> This structurally rules out a whole class of shortcuts.**

---

#### 3.3.1 Phase 1 (Write-Side): Semantic Compressor with Explanation & Reconstruction

**Role:** Semantic backbone of the block-level memory writer.  
**Goal:** Train a Q-Former to compress a 4K block into `N = 32` Detail Tokens `D` that:

- Are rich enough to reconstruct the block (no ‚Äúfingerprint loss‚Äù)  
- Are structured enough to support ‚Äúwhy retrieve / why not‚Äù style explanations  
- Are fully independent of the later indexer geometry

We use **two heads** and a carefully designed **data curriculum**.

> Input blocks at this stage are **already segmented by 3.2** (summary trigger).
> The Memory OS always takes a snapshot of the **full block hidden states / KV, before any Trimmer eviction**, and the Q-Former operates on this pre-trim snapshot.
> During training we may still simulate logical trimming for the teacher‚Äìstudent setup, but the archival snapshot `B` is always defined as the **pre-trim block**.

---

##### 3.3.1.1 Data Construction: Positive / Negative Pairs & Shortcut Control

Each training sample is a tuple:

\[
(\text{Block } B,\ \text{Query } q,\ \text{Label } y,\ \text{Metadata})
\]

where:

- `B`: a 4K token block (dialogue window, document span, code chunk, etc.)  
- `q`: a query that may or may not require information inside `B`  
- `y`: label indicating:
  - `y = 1`: **B is a supporting block** for `q`  
  - `y = 0`: `B` is **not** sufficient / relevant for `q`  
- `Metadata`: structured description (e.g., version, entity IDs, timestamps) used only for **data generation**, not exposed as input

**Key constraint** (sameÁ≤æÁ•û as 3.2, but now at block level):

We actively design data so that the model **cannot** solve the task by:

- Using only the query `q` and its prior knowledge  
- Memorizing global patterns like ‚Äúv2.0 questions almost always use later blocks‚Äù  
- Ignoring `D` and still answering correctly

Concretely:

1. **Inject block-local facts that are not in the base LLM**
   - Synthetic IDs, random hashes, opaque codes  
   - Local redefinitions: e.g., rebind `var_x` within `B`  
   - Custom ‚Äúv1.0 vs v2.0‚Äù semantics that contradict public knowledge / defaults

2. **Construct hard positives**
   - `y = 1` samples where:
     - Without `B`, the query is **unanswerable**  
     - With `B`, it becomes answerable via a small number of reasoning steps  
   - This forces the model to use `D` as the **only reliable source** for certain key facts.

3. **Construct hard negatives (two types)**
   - **Type N1 ‚Äì Semantic-but-wrong:**
     - Block `B_neg` has **similar topic / entities** as the true block,  
       but is logically incompatible:
       - Different version  
       - Different parameter  
       - Same function name but different behavior  
     - For these, the correct rationale is:
       > ‚ÄúI do **not** retrieve this because ‚Ä¶‚Äù
   - **Type N2 ‚Äì Distractor overlaps:**
     - Blocks that share some superficial patterns (names, keywords)  
       but are entirely irrelevant to the specific query.

By construction, any model that **ignores `D`** and relies on `q` priors will:

- Do well on easy cases, but  
- Be systematically wrong on these **adversarially shaped negatives**.

This creates a strong incentive to encode **block-local, non-global** information into `D`.

---

##### 3.3.1.2 Head A: Explanation SFT (Logic-Driven Head)

Given `(B, q, y)`:

1. Encode the block with Q-Former to get `D`:

   \[
   D = \text{QFormer}(B)
   \]

2. Feed the frozen LLM with:
   - The query `q`  
   - A special memory segment containing `D`, bracketed by:
     - `<MEM_START>` ‚Ä¶ `<MEM_END>`

3. Train the LLM (with a small trainable head or LoRA) to generate a **Retrieval Rationale**:

   - If `y = 1` (positive block):

     > ‚ÄúI retrieve this because it defines `var_x` used in question (2).  
     > It also introduces the v2.0 behavior that the query is asking about.‚Äù

   - If `y = 0` (negative block):

     > ‚ÄúI do **not** retrieve this because this block describes v1.0  
     > while the question asks specifically about v2.0 semantics.‚Äù

The loss `\(\mathcal{L}_\text{rationale}\)` can be:

- Pure sequence-level CE over rationale tokens, or  
- A mix of:
  - CE (for fluency)  
  - Classification head (retrieve / not retrieve)  
  - Span-level supervision (pointing to specific facts in `B`)

**Why this avoids shortcuts:**

- Hard negatives are constructed so that:
  - Top-level topic and entities can be similar, but  
  - Correctness hinges on **fine details in `B`**.  
- Any rationale that does **not** read those details from `D` will be inconsistent with label `y`.

---

##### 3.3.1.3 Head B: Self-Supervised Reconstruction (Coverage & Fingerprints)

Explanation alone is not enough.  
We also need `D` to carry **raw, non-verbalizable fingerprints**:

- hashes  
- opaque identifiers  
- arbitrary numeric patterns  
- local structural quirks

We add a **self-supervised reconstruction head**:

1. The model sees:
   - (Optionally) the query `q`  
   - Detail Tokens `D` as a prefix memory segment

2. It is trained to reconstruct a target derived from `B`, e.g.:

   - Full token sequence of `B` (causal LM loss), or  
   - A compressed representation:
     - Important spans  
     - Extracted key-value pairs  
     - Chunked tokens with span corruption / denoising

3. Loss `\(\mathcal{L}_\text{recon}\)`:

   - Token-level CE (standard LM loss)  
   - Optionally combined with a span-level objective (e.g., denoising autoencoder)

**Effect:**

- `\(\mathcal{L}_\text{recon}\)` is:
  - Self-supervised (no extra labels)  
  - A cheap ‚Äúinsurance policy‚Äù ensuring that `D` is **information-complete**  
- To minimize `\(\mathcal{L}_\text{recon}\)`, the Q-Former must encode:
  - IDs  
  - Rare tokens  
  - Local structure  
  that may **never** be mentioned in natural-language rationales.

---

##### 3.3.1.4 Phase 1 Objective & Freezing

The Phase 1 loss:

\[
\mathcal{L}_\text{Phase1} = \mathcal{L}_\text{rationale} + \lambda_\text{recon} \cdot \mathcal{L}_\text{recon}
\]

After convergence:

- Q-Former is **frozen**.  
- `D` is now:
  - Explanation-aware (via SFT)  
  - Detail-complete (via reconstruction)  
  - Independent of any particular retrieval index geometry

These Detail Tokens `D` become the canonical **Tier-2 payload** stored in **L2 Semantic Payload** (see Section 2.3).

---

#### 3.3.2 Phase 2 (Write-Side ‚Üí Index): Block Handler via Two-Tower Distillation

**Role:** Learn a **retrieval-friendly geometry** on top of the frozen semantic space `D`.  
**Goal:** Convert each block‚Äôs `D` into a single **index embedding** `H`, and align it with LLM query embeddings `Q`.

Important constraints:

- The **writer tower** sees only `D` (Phase 1 output).  
  - It does **not** see raw block text or KV.  
  - This structurally blocks a large class of shortcuts.

- The **reader tower** lives on the LLM side (read-side), but it learns purely from:
  - LLM hidden states as input,  
  - Block-level supervision (which block should be retrieved),  
  - **No direct access** to block contents.

---

##### 3.3.2.1 Writer Tower: `D ‚Üí H` (Block Handler)

- **Input:** frozen `D` for a block `B`  
- **Architecture:**
  - Tiny Transformer (2‚Äì4 layers) over the sequence of Detail Tokens  
  - Random token dropout (e.g., mask 30‚Äì50% of `D` during training) to:
    - Prevent over-reliance on a single ‚Äúgolden‚Äù token  
    - Encourage **global use** of the semantic space

- **Output:**

  \[
  H = g(D) \in \mathbb{R}^d
  \]

  a single **index embedding** per block.

`H` is what we store in **L1 Hot Index** (e.g., HNSW / DNII).

---

##### 3.3.2.2 Reader Tower: `h_seed ‚Üí Q` (LLM Query Head)

- **Input:** a **seed hidden state** `h_\text{seed}` from the LLM:

  - At a `<RETRIEVE>` token, or  
  - Constructed by fusing:
    - current position hidden  
    - relevant tombstone hidden states  
    - local context summary

- **Architecture:**
  - MLP / Mixer / small gated network

- **Output:**

  \[
  Q = f(h_\text{seed}) \in \mathbb{R}^d
  \]

`Q` lives in the **same embedding space** as `H`.

---

##### 3.3.2.3 Contrastive Training: Aligning `Q` and `H`

For each training instance, we know:

- The **supporting blocks** `B_\text{pos}` whose Detail Tokens `D_\text{pos}` contain the answer  
- A pool of negatives `B_\text{neg}`:
  - Random negatives  
  - Hard negatives:
    - same topic, wrong version  
    - same entity, wrong time  
    - same function name, different behavior

We compute:

\[
H_\text{pos} = g(D_\text{pos}), \quad
H_\text{neg} = g(D_\text{neg}), \quad
Q = f(h_\text{seed})
\]

Then use an **InfoNCE loss**:

\[
\mathcal{L}_\text{index} = - \log \frac{\exp(\text{sim}(Q, H_\text{pos}) / \tau)}
{\exp(\text{sim}(Q, H_\text{pos}) / \tau) + \sum_{H \in \mathcal{N}} \exp(\text{sim}(Q, H) / \tau)}
\]

where:

- `sim` is cosine similarity or dot product  
- `ùí©` contains in-batch negatives + mined hard negatives

This teaches:

- `Q` to land near the **correct block handlers**  
- `H` to be **discriminative** among semantically similar but logically different blocks

After Phase 2:

- `D` (Phase 1) remains frozen, stored in **L2**  
- `H` (Phase 2) forms the **L1 index** for fast ANN / DNII retrieval  
- The reader tower `f` becomes the **retrieval head** for the LLM.

---

#### 3.3.3 Read-Side (Runtime): Using `H` and `D` at Inference

With write-side semantics fixed, runtime retrieval decomposes into:

1. **When to retrieve?** ‚Äì decided by the Memory OS and `<RETRIEVE>` / tombstone mechanisms (see Section 3.2 & policy section).  
2. **How to build `Q`?** ‚Äì apply the reader tower `f` to the chosen `h_\text{seed}`.  
3. **How to reuse blocks?** ‚Äì use `H` for fast lookup, `D` for semantic injection.

At a high level:

1. **Trigger**  
   - Model emits `<RETRIEVE>`, or  
   - Tombstone attention / periodic policy decides to retrieve.

2. **Query projection**  
   - Construct `h_\text{seed}` from current / tombstone / local summary hidden states.  
   - Compute:
     \[
     Q = f(h_\text{seed})
     \]

3. **Index search (L1)**  
   - Use `Q` to query the L1 Hot Index over `{H}` (HNSW / DNII).  
   - Obtain Top-K block IDs.

4. **Load semantic payload (L2)**  
   - For each retrieved block:
     - Load its frozen Detail Tokens `D` from L2 Semantic Payload (host RAM).

5. **Inject `D` as soft prompt**  
   - Insert retrieved `D` into the context, bracketed by `<MEM_START>` / `<MEM_END>`.  
   - For a short horizon `T_\text{protect}`, protect these tokens from trimming so the model can fully use them.

6. **Optional raw KV expansion (L3)**  
   - In rare, explicit cases (verbatim quoting, oracle-style analysis),  
     - Load raw, de-rotated KV from L3 Archive,  
     - Re-rotate with RoPE,  
     - Splice into the GPU KV cache.

---

**Summary for 3.3**

- **Phase 1 (Q-Former):**  
  - Learns an **interpretation-friendly, detail-complete** semantic representation `D` for each block, using:
    - Explanation SFT (logic)  
    - Self-supervised reconstruction (coverage / fingerprints)  

- **Phase 2 (Two-Tower Indexer):**  
  - Learns a **retrieval geometry** (`H`, `Q`) *on top of `D`*, without touching raw blocks.  

- **Runtime:**  
  - Memory OS decides **when** to retrieve (`<RETRIEVE>` + tombstones).  
  - Reader tower maps LLM hidden state to `Q`.  
  - Index returns block handlers `H`; we fetch `D` and inject them as the **default recall payload**, with raw KV as an optional fallback.
